% Appendix B

\chapter{Horizontal Reweighting}
\label{AppendixB}

The current appndix addresses the issue of manipulating some input distribution, {\it source}, such that
it becomes statistically compatible with a given, {\it target}, distribution. A common method to do that
involves binning the two distributions. This method is sometimes called {\it vertical reweighting}
and it is described in the following section. The main motivation behind the techinique advertised in the current
appendix is to circumvent problems arising from binning distributions, especially when the number of
bins is big. Note that the bigger the number of bins the more precise the matching of the two distributions
is. The advertised techinique is described in the current appendix and a choice for its name could be {\it horizontal reweighting}.

Lastly an matching example is given in the last section fo this appendix. This example is a typical problem
in high energy physics originating from due non perfect simulation. Specifically certain kinematic
distributions might defer significantly between simulated events and real data. Given that simulated
events are commonly used to control acceptance, resolution or other detector effects; What is typically
done is to correct the simulated sample by matching its kinematic distributions to the ones observed in data.
An example of a situation where this problem arises can be found in \secref{Accceptance_Corrections}.

\subsubsection{Vertical Reweighting}
The vertical reweighting approach to matching distributions is straghtforward.
The first step is to bin the source (S) and targert (T) distributions with the same binning scheam.
After that a weight, $w_i$, is assigned to each event in a given bin, $i$, such that the bin
contents of the source distributions matches these of the target, see \equref{vert_rew}.
Essentially this techinique moves vertically each bin of the seource distribution in order to match
the bin contents of the target distribtuon.

\begin{equation}
  \centering
  w_i = \frac{T_{{\rm bin}i}}{S_{{\rm bin}i}}
\label{vert_rew}
\end{equation}

The advanteges of the vertical reweighting approach is that it is easy to understand and implement.
However, there are some disadvantages that result from the binning itself. For example it can happen
that a given source or target bin has zero entries for a given binning scheam. This situation gets
becomes more pronounced in the case of large number of bins, which as already mentioned improves the
precision of the matching. In additon it can also happen that any of the distributions is weighted and
the sum of weights in a given bin is negative. Both of the above situatuions require some justification
as to how these problematic cases can be handled. In addition, it can also happen that the source to
target matching needs to be done in many dimentions, as mentioned in the introduction fo the current
appendix. In that case the number of bins increase rapidly, and thus the number of problematic bins as well,
to the point that it is no longer possible to match the source distributions to the target one.
Note that variables corresponding to these dimensions are in general correlated with each other.
Hence, doing several one dimensional reweighting steps will simply ignore these correlations.

\subsubsection{Horizontal Reweighting}
The horizontal approach to matching distributuons is meant to bypass the problem of binning, especially
in many dimmenssions, and thus make it possible to match an arbitrary of variables between source and target.
The basic idea of the approach is to apply as chain of transformations to both source and target distributions
such that the become uncorrrelated. Subsequently the transformation chain applied that has been applied to
the targer distributions are inverted and then applied to the source ones. The full logic is shown in \figref{}
and it is inspired by discussions with Gerhard Raven and Diego Martinez Santos.

\begin{equation}
 S \underbrace{\rightarrow}_{\gamma_s^{\text{flat}}}  S_\text{flat} \underbrace{\rightarrow}_{\gamma_s^{\text{gaussian}}} S_{gaussian} \underbrace{\rightarrow}_{\gamma_s^{\text{gaussian uncoor}}} S_\text{gaussian uncorr} \nonumber
\end{equation}
\begin{equation}
 T \underbrace{\rightarrow}_{\gamma_t^{\text{flat}}}  T_\text{flat} \underbrace{\rightarrow}_{\gamma_t^{\text{gaussian}}} T_{gaussian} \underbrace{\rightarrow}_{\gamma_t^{\text{gaussian uncoor}}} T_\text{gaussian uncorr} \nonumber
\end{equation}
\begin{equation}
S_\text{matched} = S_\text{gaussian uncorr} \times (\gamma_t^{\text{gauusian uncorr}})^{-1} \times (\gamma_t^{\text{gauusian}})^{-1} \times (\gamma_t^{\text{flat}})^{-1} \nonumber
\end{equation}

The logic presented in \figref{} can be understood requires three distinct transformations and the orresponding
inverted transformation as well. These transformations are based in well known mathematical techiniques; Namelly
{ \it Inverse Transformation Sampling} \cite{} and {\it Matrix diagonilization} \cite{}.

\vspace{1cm}

\noindent {\it Probability Integral Transform} ($\gamma_s^{\text{flat}}$) :
\begin{center}
\begin{itemize}
\item Let $x$ be a random variable following the pdf $P$.
\item Let $F_x$ be the cdf of $P$.
\item Then the variable $Y=F_x(x)$ is uniform.
\end{itemize}
\end{center}

\begin{equation}
F_x(X) = \int_{-\infty}^X P(t)dt  = \text{prob}({\bf x} \leq X) \nonumber
\end{equation}

\noindent {\it Inverse Transformation Sampling} (${\gamma_s^{\text{flat}}}^{-1}$):
\begin{center}
\begin{itemize}
\item Let Y be a uniformly distributed variable.
\item x has a cdf, call it $F_x$.
\item Then the variable $x^\prime = F_x^{-1}(Y)$ is distributed as x.
\end{itemize}
\end{center}

Note tha the above transformation is general and can be aplpyied to any distribution
even if the distribution does not have known analytica shape.
It is usefull to point out that a binning is introduced when computing the cumulative of a
distribution during the flattening step. However this binning can be arbitrarily fine\footnote{However
from implementation point of you the larger the number of bins the slower the flatening, and inverse flatening,
procedure becomes. Here is where the built-in function {\tt numpy.digitise()} of {\tt python} proves to be
usefull, as the timming scales nicelly with the number of bins.} without  having the problems explained
in the introduction, since it is not used to much any distribution.

\vspace{1cm}


\noindent {\it From Uniform to Normal} ($\gamma_s^{\text{gaussian}}$):
\begin{center}
\begin{itemize}
\item Let $x$ be a uniformly distributed variable.
\item Then the variable $y=m + \sqrt{2} \; \sigma \; \text{ErfInverse}(x)$ is normally distributed.
\end{itemize}
\end{center}

\noindent {\it From Normal to Uniform} (${\gamma_s^{\text{gaussian}}}^{-1}$):
\begin{center}
\begin{itemize}
\item Let $x$ be a uniformly distributed variable.
\item Then the variable $u= \frac{\text{ErfComp}\left( (x - m)/\sqrt{2}\sigma \right)}{2}$ is uniformly distributed.
\end{itemize}
\end{center}


\vspace{1cm}


\noindent {\it De-correlate Variables aka Diagonalization}:
\begin{center}
\begin{itemize}
\item Let $\vec{x}$ be a set of correlated variables.
\item Let $C$ be the covariance matrix of $\vec{x}$.
\item Let $P^{-1}$ be the matrix that has the eigen-vectors of $C$ as columns.
\item ($\gamma_t^{\text{gauusian uncorr}}$): The set of values $\vec{x^\prime} = P\vec{x}$ is an uncorrelated set of $\vec{x}$. .
\item (${\gamma_t^{\text{gauusian uncorr}}}^{-1}$): The set of values $\vec{x} = P^{-1}\vec{x}$ is the corresponding correlated set of $\vec{x^{\prime}}$.
\end{itemize}
\end{center}

\noindent The correlation betwen varible sis computed usign the standard formula of \equref{hor_reww_correlation_formula}

\begin{equation}
\centering
c_{ij} = \frac{1}{N} \sum (x_i-<x_i>)(x_j-<x_j>), \;\; \text {with} <x> = \frac{1}{N} \sum x \nonumber
\label{hor_reww_correlation_formula}
\end{equation}

The necessary ingridients necessary to apply the logic of \figref{} have been presented.
An implementation of the full matching algorithm can be found in github here blahhh.

\subsubsection{Example-Discussion}
To illustrate that the sdvertised techinique works a typical problem in high energy physics is addressed.
Specifically, the \Kstarz particle from the \BsJpsiKst mode decays into a \kaon and a \pion. The momenta
distributions of these two particles $\parenthesis{\ptot(\kaon)-\ptot(\pion)}$ is found to defer betwen
monte carlo (source) and background subtracted real data (target), see \figref{rew_original}. Furthermore,
the previous distributions are correlated and combined with a fine binning will yield problems as explained
earlier in the veticl reweighting subsection. After appling the technique advertised before the
$\parenthesis{\ptot(\kaon)-\ptot(\pion)}$ distributions statistically comaptible, bypassing the risks associated
to a multidimensional fine binning scheam. The matched distributions are shown in \figref{}. Furthermore,
a Kolmogorov-Smirinov test is performed to quantify the matching of each source-target distribution.
The results are summarised in \tabref{hor_rew_ks_test}.

\begin{figure}[t]
  \centering
  \begin{subfigure}{0.5\textwidth}
    \raggedright
    \tikzsetnextfilename{kplus_original}
    \scalebox{1.15}{\input{Figures/Appendix/Kplus_P_original}}
    \caption{}
    \label{kplus_rew_original}
  \end{subfigure}%
  \hfill
  \begin{subfigure}{0.5\textwidth}
    \raggedleft
    \tikzsetnextfilename{pminus_original}
    \scalebox{1.15}{\input{Figures/Appendix/piminus_P_original}}
    \caption{}
    \label{pminus_rew_original}
  \end{subfigure}
\begin{subfigure}{0.5\textwidth}
    \raggedright
    \tikzsetnextfilename{kplus_matched}
    \scalebox{1.15}{\input{Figures/Appendix/Kplus_P_matched}}
    \caption{}
    \label{kplus_rew_matched}
  \end{subfigure}%
  \hfill
  \begin{subfigure}{0.5\textwidth}
    \raggedleft
    \tikzsetnextfilename{pminus_matched}
    \scalebox{1.15}{\input{Figures/Appendix/piminus_P_matched}}
    \caption{}
    \label{pminus_rew_matched}
  \end{subfigure}
  \caption{Comparision before and after matching. Source(Target) distributions are shown in red(blue) color.
   The upper(lower) two distributions are the original(matched) distributions.}
  \label{rew_original}
\end{figure}

\begin{table}[!h]
  \center
  \begin{tabular}{c c c}
    \hline
      distribution   & KS before matching  & KS after matching \\
      \hline
       \ptot(\kaon)   &  $10^{-9}$   & $0.998$ \\
       \ptot(\pion)   &  $10^{-25}$  & $1.000$ \\
      \hline
  \end{tabular}
  \caption{\small KS test values between source and target for each of the two distributions $\parenthesis{\ptot(\kaon)-\ptot(\pion)}$
           before and after matching. The KS values indicate that the agrement after matchin improves significantly.}
  \label{hor_rew_ks_test}
\end{table}
