% Appendix B

\chapter{Horizontal Reweighting}
\label{AppendixB}

The current appndix addresses the issue of manipulating some input distribution, {\it source}, such that
it becomes statistically compatible with a given, {\it target}, distribution. A common method to do that
involves binning the two distributions. This method is sometimes called {\it vertical reweighting}
and it is described in the following section. The main motivation behind the techinique advertised in the current
appendix is to circumvent problems arising from binning distributions, especially when the number of
bins is big. Note that the bigger the number of bins the more precise the matching of the two distributions
is. The advertised techinique is described in the current appendix and a choice for its name could be {\it horizontal reweighting}.

Lastly an matching example is given in the last section fo this appendix. This example is a typical problem
in high energy physics originating from due non perfect simulation. Specifically certain kinematic
distributions might defer significantly between simulated events and real data. Given that simulated
events are commonly used to control acceptance, resolution or other detector effects; What is typically
done is to correct the simulated sample by matching its kinematic distributions to the ones observed in data.
An example of a situation where this problem arises can be found in \secref{Accceptance_Corrections}.

\subsubsection{Vertical Reweighting}
The vertical reweighting approach to matching distributions is straghtforward.
The first step is to bin the source (S) and targert (T) distributions with the same binning scheam.
After that a weight, $w_i$, is assigned to each event in a given bin, $i$, such that the bin
contents of the source distributions matches these of the target, see \equref{vert_rew}.
Essentially this techinique moves vertically each bin of the seource distribution in order to match
the bin contents of the target distribtuon.

\begin{equation}
  \centering
  w_i = \frac{T_{{\rm bin}i}}{S_{{\rm bin}i}}
\label{vert_rew}
\end{equation}

The advanteges of the vertical reweighting approach is that it is easy to understand and implement.
However, there are some disadvantages that result from the binning itself. For example it can happen
that a given source or target bin has zero entries for a given binning scheam. This situation gets
becomes more pronounced in the case of large number of bins, which as already mentioned improves the
precision of the matching. In additon it can also happen that any of the distributions is weighted and
the sum of weights in a given bin is negative. Both of the above situatuions require some justification
as to how these problematic cases can be handled. In addition, it can also happen that the source to
target matching needs to be done in many dimentions, as mentioned in the introduction fo the current
appendix. In that case the number of bins increase rapidly, and thus the number of problematic bins as well,
to the point that it is no longer possible to match the source distributions to the target one.
Note that variables corresponding to these dimensions are in general correlated with each other.
Hence, doing several one dimensional reweighting steps will simply ignore these correlations.

\subsubsection{Horizontal Reweighting}
The horizontal approach to matching distributuons is meant to bypass the problem of binning, especially
in many dimmenssions, and thus make it possible to match an arbitrary of variables between source and target.
The basic idea of the approach is to apply as chain of transformations to both source, \equref{source_trans},
and target, \equref{target_trans}, distributions such that they become uncorrrelated. Subsequently the
transformation chain that has been applied to the target distributions are inverted and then applied to the
source ones, \equref{match_distributions}. The function of three transforamtions involded is:
Transformation $(A)$ converts the input distribution to flat. Transformation $(B)$ converts a flat distribution
to a normal distribution, while the last $(C)$ converts removes the correlation between two distributions.
The necessary mathematical tool are presented in the next paragraph. The logic is inspired by discussions
with Gerhard Raven and Diego Martinez Santos.

\begin{equation}
  \label{source_trans}
  S \times \gFlat{S} \rightarrow S_{\rm flat} \times \gGaus{S} \rightarrow S_{\rm gaus} \times \gGausUn{S} \rightarrow S_\text{\rm uncor gaus}
\end{equation}
\begin{equation}
  \label{target_trans}
  T \times \gFlat{T} \rightarrow T_{\rm flat} \times \gGaus{T} \rightarrow T_{\rm gaus} \times \gGausUn{T} \rightarrow T_\text{\rm uncor gaus}
\end{equation}
\begin{equation}
  \label{match_distributions}
  S_{\rm matched} = S_{\rm uncor gaus} \times \gGausUnInv{T} \times \gGausInv{T} \times \gFlatInv{T}
\end{equation}

The logic presented above requires three distinct steps-transformations and the corresponding
inverted ones. These transformations make use of two well known mathematical techiniques.
Namelly, { \it Inverse Transformation Sampling} and {\it Matrix diagonilization}.

\noindent The {\it Probability Integral Transform} theorem states that:
\begin{center}
\begin{itemize}
\item Let $x$ be a random variable following the pdf $P$.
\item Let $F_x$ be the cdf of $P$.
\item Then the variable $Y=F_x(x)$ is uniform.
\end{itemize}
\end{center}

\noindent The cdf is defined as:
\begin{equation}
F_x(X) = \int_{-\infty}^X P(t)dt  = \text{prob}({\bf x} \leq X) \nonumber
\end{equation}

\noindent The innverted version of the previous theorem states that {\it Inverse Transformation Sampling}:
\begin{center}
\begin{itemize}
\item Let Y be a uniformly distributed variable.
\item x has a cdf, call it $F_x$.
\item Then the variable $x^\prime = F_x^{-1}(Y)$ is distributed as x {\color{red} for small x steps, check!!!}.
\end{itemize}
\end{center}

\noindent The above theorem applies to the \gFlat{}, while the transformation \gGaus{} is
a special case of the same theorem. Note tha the shape of the initial source or target distributions are
not required to have a known analytica shape for their cdf to be computed. Insted these distributons can
be binned so that the cumulative sum in each bin can be computed. It is important to point out that this
binning can be arbitrarily fine\footnote{However from implementation point of you the larger the number of
bins the slower the flatening, and inverse flatening, procedure becomes. Here is where the built-in function
{\tt numpy.digitise()} of {\tt python} proves to be usefull, as the timming scales nicelly with the number of bins.}
without having the problems explained in the introduction, since it is not used to much any distribution.

% \vspace{1cm}
%
% \noindent {\it From Uniform to Normal} ($\gTransS{flat}$):
% \begin{center}
% \begin{itemize}
% \item Let $x$ be a uniformly distributed variable.
% \item Then the variable $y=m + \sqrt{2} \; \sigma \; \text{ErfInverse}(x)$ is normally distributed.
% \end{itemize}
% \end{center}
%
% \noindent {\it From Normal to Uniform} (${\gTransS{flat}}^{-1}$):
% \begin{center}
% \begin{itemize}
% \item Let $x$ be a uniformly distributed variable.
% \item Then the variable $u= \frac{\text{ErfComp}\left( (x - m)/\sqrt{2}\sigma \right)}{2}$ is uniformly distributed.
% \end{itemize}
% \end{center}

The steps for obtaining a set of uncorrelated values from an initial set of correlated gaussian distributed
variables are based on the concept of {\it Matrix diagonalization} and are shown in the following:

\begin{center}
\begin{itemize}
\item Let $\vec{x}$ be a set of correlated variables.
\item Let $C$ be the covariance matrix of $\vec{x}$.
\item Let $P^{-1}$ be the matrix that has the eigen-vectors of $C$ as columns.
\item ($\gGausUn{}$): The set of values $\vec{x^\prime} = P\vec{x}$ is an uncorrelated set of $\vec{x}$. .
\item (${\gGausUn{}}^{-1}$): The set of values $\vec{x} = P^{-1}\vec{x}$ is the corresponding correlated set of $\vec{x^{\prime}}$.
\end{itemize}
\end{center}

\noindent The correlation betwen varible sis computed usign the standard formula of \equref{hor_reww_correlation_formula}

\begin{equation}
\centering
c_{ij} = \frac{1}{N} \sum (x_i-<x_i>)(x_j-<x_j>), \;\; \text {with} <x> = \frac{1}{N} \sum x \nonumber
\label{hor_reww_correlation_formula}
\end{equation}

The necessary ingridients necessary to apply the techinique presented have been covered.
An implementation of the full matching algorithm can be found in github here blahhh.

\subsubsection{Example-Discussion}
To illustrate that the sdvertised techinique works a typical problem in high energy physics is addressed.
Specifically, the \Kstarz particle from the \BsJpsiKst mode decays into a \kaon and a \pion. The momenta
distributions of these two particles $\parenthesis{\ptot(\kaon)-\ptot(\pion)}$ is found to defer betwen
monte carlo (source) and background subtracted real data (target), see the top two plots of \figref{hor_rew_example_figs}.
Furthermore, the previous distributions are correlated and combined with a fine binning will yield problems as explained
earlier in the veticl reweighting subsection. After appling the technique advertised before the
$\parenthesis{\ptot(\kaon)-\ptot(\pion)}$ distributions statistically comaptible, bypassing the risks associated
to a multidimensional fine binning scheam. The matched distributions can be seen at the bottom of \figref{hor_rew_example_figs}.
In addition, a Kolmogorovâ€“Smirnov test is performed to quantify the matching of each source-target distribution.
The results are summarised in \tabref{hor_rew_ks_test}.

\begin{figure}[t]
  \centering
  \begin{subfigure}{0.5\textwidth}
    \raggedright
    \tikzsetnextfilename{kplus_original}
    \scalebox{1.15}{\input{Figures/Appendix/Kplus_P_original}}
    \caption{}
    \label{kplus_rew_original}
  \end{subfigure}%
  \hfill
  \begin{subfigure}{0.5\textwidth}
    \raggedleft
    \tikzsetnextfilename{pminus_original}
    \scalebox{1.15}{\input{Figures/Appendix/piminus_P_original}}
    \caption{}
    \label{pminus_rew_original}
  \end{subfigure}
\begin{subfigure}{0.5\textwidth}
    \raggedright
    \tikzsetnextfilename{kplus_matched}
    \scalebox{1.15}{\input{Figures/Appendix/Kplus_P_matched}}
    \caption{}
    \label{kplus_rew_matched}
  \end{subfigure}%
  \hfill
  \begin{subfigure}{0.5\textwidth}
    \raggedleft
    \tikzsetnextfilename{pminus_matched}
    \scalebox{1.15}{\input{Figures/Appendix/piminus_P_matched}}
    \caption{}
    \label{pminus_rew_matched}
  \end{subfigure}
  \caption{Comparision before and after matching. Source(Target) distributions are shown in red(blue) color.
   The upper(lower) two distributions are the original(matched) distributions.}
  \label{hor_rew_example_figs}
\end{figure}

\begin{table}[!h]
  \center
  \begin{tabular}{c c c}
    \hline
      distribution   & KS before matching  & KS after matching \\
      \hline
       \ptot(\kaon)   &  $10^{-9}$   & $0.998$ \\
       \ptot(\pion)   &  $10^{-25}$  & $1.000$ \\
      \hline
  \end{tabular}
  \caption{\small KS test values between source and target for each of the two distributions $\parenthesis{\ptot(\kaon)-\ptot(\pion)}$
           before and after matching. The KS values indicate that the agrement after matchin improves significantly.}
  \label{hor_rew_ks_test}
\end{table}
