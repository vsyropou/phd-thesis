% Appendix B

\chapter{Horizontal Weighting}
\label{AppendixB}

The current appendix addresses the issue of manipulating some input distribution, {\it source}, such that
it becomes statistically compatible with a given, {\it target}, distribution. A common method to do that
involves binning the two distributions. This method is sometimes called {\it vertical weighting}
and it is described in the following section. The main motivation behind the technique advertised in the current
appendix is to circumvent problems arising from binning distributions, especially with increasing number of
bins or in case of small sample sizes. Note that the bigger the number of bins the more precise the matching of the two distributions
is. The advertised technique is described in the current appendix and a choice for its name could be {\it horizontal weighting}.
The technique is inspired by discussions with Gerhard Raven and Diego Martinez Santos.

Lastly a matching example is given at the of the current appendix. This example is a typical problem
in high energy physics originating from non perfect simulation. Specifically certain kinematic
distributions might defer significantly between simulated events and data. Given that simulated
events are commonly used to control acceptance, resolution or other detector effects; what is typically
done is to correct the simulated data sample by matching its kinematic distributions to the ones observed in data.
An example of a situation where this problem arises can be found in \secref{Accceptance_Corrections}.

\subsubsection{Vertical Weighting}
The vertical weighting approach to matching distributions is straightforward.
The first step is to bin the source (S) and target (T) distributions with the same binning scheme.
After that a weight, $w_i$, is assigned to each entry in a given bin, $i$, such that the bin
contents of the source distributions matches these of the target, see \equref{vert_rew}.
Essentially this technique moves vertically each bin of the source distribution in order to match
the bin contents of the target distribution.

\begin{equation}
  \centering
  w_i = \frac{T_{{\rm bin}i}}{S_{{\rm bin}i}}
\label{vert_rew}
\end{equation}

The advantages of the vertical weighting approach is that it is easy to understand and implement.
However, there are some disadvantages that result from the binning itself. For example it can happen
that a given source or target bin has zero entries for a given binning scheme. This situation
becomes more pronounced in the case of large number of bins, which as already mentioned improves the
precision of the matching. In addition it can also happen that any of the distributions is weighted and
the sum of weights in a given bin is negative. Both of the above situation require some justification
as to how these problematic cases can be handled. In addition, it can also happen that the source to
target matching needs to be done in many dimensions, as mentioned in the introduction of the current
appendix. In that case the number of bins increase rapidly, and thus the number of problematic bins as well,
to the point that it is no longer possible to match the source distributions to the target one.
Note that variables corresponding to these dimensions are in general correlated with each other.
Hence, doing several one dimensional weighting steps will simply ignore these correlations.

\subsubsection{Horizontal Weighting}
The horizontal approach to matching distributions is meant to bypass the problem of binning, especially in many dimensions,
and thus make it possible to match an arbitrary number of variables between source
and target. The basic idea of the approach is to apply as chain of transformations to both source and target
distributions, such that they become uncorrelated, see \equref{source_trans} and \equref{target_trans}
respectively. Subsequently the transformation chain that has been applied to the target distributions
are inverted and then applied to the source ones, \equref{match_distributions}. The three transformations
involved in the current approach are: Transformation $(A)$ converts the input distribution to flat.
Transformation $(B)$ converts a flat distribution to a normal distribution while $(C)$ removes
the correlation between two distributions. The necessary mathematical tools to perform the above transformations
are presented in the next paragraph.

\begin{subequations}
\begin{equation}
  \label{source_trans}
  S \times \gFlat{S} \rightarrow S_{\rm flat} \times \gGaus{S} \rightarrow S_{\rm gaus} \times \gGausUn{S} \rightarrow S_\text{\rm uncor gaus}
\end{equation}
\begin{equation}
  \label{target_trans}
  T \times \gFlat{T} \rightarrow T_{\rm flat} \times \gGaus{T} \rightarrow T_{\rm gaus} \times \gGausUn{T} \rightarrow T_\text{\rm uncor gaus}
\end{equation}
\begin{equation}
  \label{match_distributions}
  S_{\rm matched} = S_{\rm uncor gaus} \times \gGausUnInv{T} \times \gGausInv{T} \times \gFlatInv{T}
\end{equation}
\label{hor_rew_logic}
\end{subequations}

The transformation steps, \gFlat{} and \gGaus{}, that appear in the above logical steps make use of
well known mathematical theorems, namely {\it Inverse Transformation Sampling} and {\it Probability
Integral Transform}. No prof of the above theorems is included since the intention of the current
appendix is to quickly demonstrate the advertised technique. However, there is plethora of examples online.

\begin{theorem}[Inverse Transformation Sampling]
  Let $u$ be a uniformly distributed variable. Consider another random variable $x$ that has a Cumulative
  Distribution Function (\cdf), call it $F_x$. Then the variable $x^\prime = F_x^{-1}(u)$ is distributed the
  same way as $x$ does.
  \label{theo_inv_trans_sampling}
\end{theorem}

\begin{theorem}[Probability Integral Transform]
  Let $x$ be a random variable distributed according to a \pdf, $P(x)$. Let $F_x$ be the \cdf of $P(x)$.
  Then the variable $u=F_x(x)$ is uniform. This theorem is the inverse of \theoref{theo_inv_trans_sampling}.
  \label{theo_prob_inte_trans}
\end{theorem}

\noindent For completion the \cdf, $F_x(y)$, of a certain \pdf, $P(x)$, where $y$ takes its values from the same
domain as $x$, is by definition:

\begin{equation}
F_x(y) = \int_{-\infty}^y P(t)dt  = {\rm Probability}(x \leq y)
\label{cdf_def}
\end{equation}

Both of the above theorems can only be directly applied to continuous distributions.
However, exact analytic shape of the \pdfs or \cdfs involved is not required. This is because the \cdf
of any variable $x$ can be built by essential computing the integral-sum of \equref{cdf_def}. In order
to do that $x$ is binned so that the cumulative sum of each bin can be computed. This is a straightforward
step, where the number of entries in each bin are added to the number of entries of the next bin until the
range of $x$ is exhausted. It is important to point out that this binning can be arbitrarily fine without
having the problems explained in the introduction, since it is not used to much any distribution.
However, from implementation point of you the larger the number of bins the slower the algorithm performs.
(Here is where the built-in function {\tt numpy.digitise()} of {\tt python} proves to be useful, as the timing scales
nicely with the number of bins.)

The transformations \gFlat{} and \gGaus{} mentioned in \equref{hor_rew_logic}, are direct implementations of
\theoref{theo_prob_inte_trans}. Similarly the inverted versions, \gFlatInv{} and \gGausInv{},
of the previous transformations are direct implementations of \theoref{theo_inv_trans_sampling}.
The only deference between is case of the transformations \gGaus{} and \gGausInv{} the shapes
of the \pdf and \cdf involved are known analytically. The previous transformations involve the
{\it inverse error function} and the {\it complementary error function} respectively, which
are both well known distributions.

Coming now to the last transformation step, \gGausUn{}, of \equref{hor_rew_logic}.
This step essentially performs a linear transformation to two correlated Gaussian distributed
variables such that they become uncorrelated. The method followed is essentially a standard
{\it Matrix diagonalization}; The relevant steps required are described in \methref{meth_matrix_diag}

\begin{method}
 Let $\vec{x}$ be a set of correlated variables, with a corresponding covariance matrix $C$.
 Let $P^{-1}$ be the matrix that has the eigen-vectors of $C$ as columns.
 The set of values $\vec{x^\prime} = P\vec{x}$ is an uncorrelated set of $\vec{x}$.
 The set of values $\vec{x} = P^{-1}\vec{x}$ is the corresponding correlated set of $\vec{x^{\prime}}$.
 \label{meth_matrix_diag}
\end{method}

\noindent The covariance between variables is computed using the standard formula of \equref{hor_reww_correlation_formula}

\begin{equation}
\centering
c_{ij} = \frac{1}{N} \sum (x_i-<x_i>)(x_j-<x_j>), \;\; \text {with} <x> = \frac{1}{N} \sum x \nonumber
\label{hor_reww_correlation_formula}
\end{equation}

\noindent Finding the eigen-vectors of $C$ is a bit more lengthy to quote in the current appendix.
Nevertheless, it is a straightforward well known problem for which there are many implemented
algorithms as well.

Lastly an important implementation issues is clarified. Specifically, the \cdf is built by binning a
given distribution, $x$, and associating the a value of the integral of \equref{cdf_def}, name it $c_i$,
to each bin $x_i$. In that case  the \cdf is just an $x_i \to c_i$ {\it map}
structure, implying that the $x \to c$ function is not continuous by definition. Of the binning can be increased
but one could argue that this is more brute force that solving the problem. The way to make the $x \to c$ mapping
continues is using the well known technique of {\it linear interpolation}, shown in \figref{linear_interpolation},
and thus solving the first of the two implementation issues.

\begin{equation}
  c(x) = v_i + \frac{c_{i+1}-c_i}{x_{i+1}-x_i} *  \parenthesis{x - x_i}
  \label{linear_interpolation}
\end{equation}

The ingredients necessary to apply the horizontal weighting technique presented have been covered.

\subsubsection{Example-Discussion}
To demonstrate the advertised technique a typical problem in high energy physics is addressed.
Specifically, the \Kstarz particle from the \BsJpsiKst mode decays into a \kaon and a \pion. The momenta
distributions of these two particles $\parenthesis{\ptot(\kaon)-\ptot(\pion)}$ is found to defer between
the simulation sample (source) and background subtracted data (target).
Furthermore, the previous distributions are correlated and combined with a fine binning will yield problems as explained
earlier in the vertical weighting subsection.

\begin{figure}[!t]
  \centering
  \begin{subfigure}{0.5\textwidth}
    \raggedright
    \tikzsetnextfilename{kplus_original}
    \scalebox{1.15}{\input{Figures/Appendix/Kplus_P_original}}
    %\caption{}
    \label{kplus_rew_original}
  \end{subfigure}%
  \hfill
  \begin{subfigure}{0.5\textwidth}
    \raggedleft
    \tikzsetnextfilename{pminus_original}
    \scalebox{1.15}{\input{Figures/Appendix/piminus_P_original}}
  %  \caption{}
    \label{pminus_rew_original}
  \end{subfigure}
\begin{subfigure}{0.5\textwidth}
    \raggedright
    \tikzsetnextfilename{kplus_matched}
    \scalebox{1.15}{\input{Figures/Appendix/Kplus_P_matched}}
  %  \caption{}
    \label{kplus_rew_matched}
  \end{subfigure}%
  \hfill
  \begin{subfigure}{0.5\textwidth}
    \raggedleft
    \tikzsetnextfilename{pminus_matched}
    \scalebox{1.15}{\input{Figures/Appendix/piminus_P_matched}}
  %  \caption{}
    \label{pminus_rew_matched}
  \end{subfigure}
  \caption{Comparison before and after matching. Source (Target) distributions are shown in red (blue) color.
   The upper (lower) two distributions are the original (matched) distributions.}
  \label{hor_rew_example_figs}
\end{figure}


\begin{table}[!h]
  \center
  \begin{tabular}{c c c}
    \hline
      distribution   & KS before matching  & KS after matching \\
      \hline
       \ptot(\kaon)   &  $10^{-9}$   & $0.998$ \\
       \ptot(\pion)   &  $10^{-25}$  & $1.000$ \\
      \hline
  \end{tabular}
  \caption{\small KS test between source and target for each of the two distributions $\parenthesis{\ptot(\kaon)-\ptot(\pion)}$.
  Better agreement is achieved after matching.}
  \label{hor_rew_ks_test}
\end{table}


After applying the advertised technique the
$\parenthesis{\ptot(\kaon)-\ptot(\pion)}$ distributions become statistically compatible, avoiding the risks associated
to a multidimensional fine binning scheme. The matched distributions can be seen at the bottom of \figref{hor_rew_example_figs}.
In addition, a Kolmogorov–Smirnov test is performed to quantify the matching of each source-target distribution.
The results are summarized in \tabref{hor_rew_ks_test}.
