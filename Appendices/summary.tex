\chapter*{Summary}
\chaptermark{}
\addcontentsline{toc}{chapter}{Summary - Samenvatting}

{\Large\bf
  The Weak Phase \phis and Penguin Topologies.
}
\vspace*{0.05\textwidth}

Within the domain of modern pysics there exists a particular field of research
that attempts to answer a fundamental question about the natural world. This
question follows intuitivelly when observing the natural world and it troubled
philospphers since the ancient times. These philosophers attempted to understand
which are the fundamental blocks that build the world around them. Remarkably
enough after more that 2000 years since Democitus, who is commonly accepted as
the first person that introduced the idea of idivisible blocks of nature named
atoms (or \textgreek{άτομα} in Greek), contemprary scientists still have not found a definite
answer on the fundamental blocks of nature. So far, it seems that the observable
universe consists of a handful of elementary particles, which are clasified in two
distinct categories, namely {\it gauge bosons}, responsible for mediating all the
known fundamental forces of nature (with the exception of gravity) and {\it fermions}
which are the constituents of matter. There are 5 gauge bosons and 12 fermions.
Fermions can be divided further into {\it quarks}, which constitute composite
particles like the proton. A typical {\it lepton} is the electron which orbits
the nucleus of all atoms.

\subsubsection{Particle Physics and The Standard Model}
The state of the art mathematical framework necessary to describe the interactions between the
fermions is called the \textit{Standard Model} of Particle Physics \cite{sm-glashow,sm-weinberg,sm-salam}.
Describing an interaction in this context means being able to predict the probability for a certain
outcome in a (fundamental) particle colision. Given the fact that energy and mass can convert
from one to the other, the type of the initial and final particles is not the same. For example,
two initial quarks can colide, or more precicely annihilate, and produce an electron-positron pair.
This is counter intuitive compared to the clasical, non-quantum, description of particle collisions,
where the atomic structure of particles does not change. Thus, the established predictive power of
the Standard Model is an important achievement of particle physicists. Furthermore, the recently
discovered Higgs boson \cite{higgs-cms,higgs-atlas}, which plays a special role in explaining how
particles acquire mass, makes the Standard Model robust.

Despite the its sucess in describing the outcome of particle interactions, there are certain
established phenomena and observations that Standard Model does not account for.
Perhaps the most striking one is the absence of any description about the most familiar,
yet the weakest, force of nature, meaning gravity. Or perhaps the observed matter-antimatter
asymmetry in the universe \cite{more-cpv-huet,more-cpv-gavela_I,more-cpv-gavela_II}.
Note that antimatter is a well understood state of matter that has its quantum numbers sign,
such as electric charge, flipped with respect to matter. The above phenomena are a few
examples that reveal the incompletness of the model. Thus the scientific method compels
scientists to continue testing Standard Model predictions and look for ways to improve it.
Any significant diviation from these predictions could hint the presence of
{\it New Physics} beyond the established model.


\subsubsection{The \lhcb Detector}
While from a phenomenological point of view scientists invent new observables that succefully probe
effects deviating from the Standard Model predictions, it is the role of experimental physicists to
constantly improve their infrastructure. Utilizing state of the art technologies physicists have
built the most powerfull accelerator, \lhc, at \cern. This machine is capable of accelrating protons
up to almost the speed of light and collide them at a very a high energy. To put this into perspective
the energy density of a single proton-proton collision at \lhc is approximatelly as high as when the
universe was $10^-10 \sec$ old. This effectivelly allows particle physicsc to look back in time and
probe information about the state of matter, and antimatter, in the early universe.

The \lhcb experiment, located at \cern, is a dedicated experiment. The design of the \lhcb detector,
both at the hardware and software level, is optimised for detecting the interactions of the so called
{\it heavy} quarks, like the \bquark and the \cquark quarks. It is intrasting to point out two specific
milestones of the \lhcb collaboration. The first one involves the excelent spacial resolution of particle
trajectories which is due to the \lhcb places its first active detector element closer to the \lhc beam
pipe than any other detector. Note that high spacial resolution directy increases the sensitivity to most
of the physics releated parameters that the \lhcb is targeting. The second novel techinique is the
{\it online detector alignment and calibration} which essentially enables the output data quality of the
trigger system to be as high as posible, elliminating the need of storing and processing triggered data
after detector calibration is done. This saves valuable time and resourses and at the same time allows
to increase the output data banwidth, which will eventually reduce the statistical uncertainty in all
\lhcb measurements.

Muons are important for the \lhcb experiment and are easy to identify since they penetrate matter deeply.
Given that a large fraction of the \lhcb physics program is based on the identification of muons in the
final state: for example the measurement of \phis through \BsJpsiPhi decays. It is therefore of major
importance to maintain and improve the efficiency and purity of identified muons. In addition, newly
introduced decay channels to the \lhcb physics program involve muons with low, $\pt < 0.5 \gevc$,
tranverse momentum, \eg \Sigmapmumu \cite{LHCB-CONF-2016-013-001} and \Ksmumu \cite{LHCb-CONF-2016-012}.
These type of decays are promising probes to New Physics and \lhcb has the potential to profit from a
potential lowering of the $0.5 \gevc$ thereshod that was present untill the end of the first \lhc run.


A dedicated study to explore the posibilities of recording muons with low transverse
moementum was performed. The study essentially showed that by exploiting the newly introduced charge
information of muons it is indeed possible to push the transverse momentum threashold lower. The results
are summarised in \figref{}; where the efficiency of detecting a muon is plotted against its transverse
momemntum. Note that the performance in terms of time between the default (red) and the upgraded (blue)
muon identification algorithms, is the same.

%The only limitation to lowering the $\pt$ thereshold comes  from the persence of background ammount of
\begin{figure}[t]
  \begin{subfigure}{0.5\textwidth}
    \raggedright
    \scalebox{.6}{\input{Figures/Chapter3/pt_zoom_efficiencies}}
    \label{app_eff_pt_zoom_comp}
  \end{subfigure}%
  \hfill
  \begin{subfigure}{0.5\textwidth}
    \raggedleft
    \scalebox{.6}{\input{Figures/Chapter3/p_efficiencies}}
    \label{app_eff_p_comp}
  \end{subfigure}
    \caption{Efficiency comparison between the old (red) and the upgraded (blue) algorithms.}
  \label{app_eff_comp}
\end{figure}


\subsubsection{\CP Violation}
According to the dominant consensus regarding the matter-antimatter balance in the
universe, it is believed that during the big bang matter and antimatter existed at
equal proportions. This idea invites the notion of the so called Charge-Parity (\CP)
symmetry between matter and antimatter. Perfect \CP symmetry when it comes to particle
interactions imply that nature interacts both with matter and antimatter at the same rate.
However, at current time the observable universe seems to be almost entirely populated by
matter. Given this observation, the origin of this violation of the \CP symmetry follows
as a natural question.

The established idea that nature favours processes where the final particles are matter
and not antimatter particles is indeed captured by the Standard Model. It addition, it
is intreasting to point out that the origin of \CP violation is very closely releated
to the mechanism that the fermions aquire mass, which is the Higgs boson. However,
despite the fact that the Standard Model allows for the presence of \CP violation,
it cannot account for the observed ammount of matter-antimatter asymmetry.
Hence, other sources of \CP violation beyond the Standard Model have to be active.

The ammount of \CP violation is typically quantified by several parameters for which
the predicted by the Standard Model values have to be compared against the experimentally
measured values. Potential descrepancy between the prediction and the measured value
provide hints for the presence of New Physics. It is intreasting to point out that all these paramters have to yield
a consistent picture with each other. This offers a powerful discrimination between alternative models that might
modify Standard Model predictions. In other words these alternative models have to
modify all the \CP violating parameters in a consistent way.


\subsubsection{The Weak Phase \phis}
Violation of the \CP symmetry in particle physics typicaly manifests itself when comparing
the rate, or probability, of two particle decays. The two decays are releated by a \CP symmetry.
In other words one decay consits of matter while the other of antimatter particles.
Given the established presence of \CP violation in nature the previous rates are not equal.

An intreasting particle decay to measure \CP violation and compare it with the Standard Model
predicion is the \BsJpsiPhi decay.
% This particular decay is appealing since the prediction for the amount of \CP
% violation is very precise. In addition, the \BsJpsiPhi decay has a high signal purity as well
% as a high detection efficiency, making it appealing from an experimental point of view.
Furthermore, the way \CP violation manifests itself in this parituclar decay is intriguing
as it involves the quantum mechanical interference of two decay amplitudes, see \equref{app_interf_amp}.
Note that the intereference is possible since both the \Bs particle and its antimatter counterpart,
\Bsb, can decay to the same final state $f$, as shown in \figref{app_interference}.

\newcommand{\aOne}{\ensuremath{ \mathrm{A_1}}\xspace}
\newcommand{\aTwo}{\ensuremath{ \mathrm{A_2}}\xspace}
\begin{equation}
  \centering
A(\BsJpsiPhi)^2 = |\aOne + \aTwo|^2 = |\aOne|^2 + |\aTwo|^2 + \aOne\aTwo\cos(\phis + \delta),
  \label{app_interf_amp}
\end{equation}

\noindent where the decay paths $\Bs\to f$ and $\Bs \to \Bsb \to f$ correspond to \aOne and \aTwo
respectivelly. Note the \phis parameter appearing in the interference terms of the total amplitude.
The other phase, $\delta$, is phase introduced by the strong itneraction of quarks and it does not
accountable for any \CP violating effect.

\begin{figure}[h]
  \centering
  \resizebox{0.4\textwidth}{!}{\input{Figures/Chapter1/decay}}
  \caption{The two interfering decay paths leading to the same final state.}
  \label{app_interference}
\end{figure}

The parameter quantifing \CP violation in the \BsJpsiPhi decay is the so called weak phase \phis.
The Standard Model prediction of the latter parameter as well as its most precice measurement by
\lhcb is:

\begin{subequations}
  \label{app_phis_lhcb_theo}
  \begin{align}
  \centering
  \phiS{\lhcb}           &=  -0.010 \pm 0.039(\text{total})  \;\; \text{rad},
  \label{app_phis_lhcb}\\
  \phiS{SM,tree}  &= -0.03761 {}^{+0.00073}_{-0.00082}  \;\; \text{rad}.
  \label{app_phis_theo}
\end{align}
\end{subequations}

\noindent The goal of measuring \phis is to look for New Physics effects that Standard Model
does not predict. Given the measuremnt of \equref{app_phis_lhcb} it follwos that \phis is comaptible
with the prediction and any New Physics effects that might apear in \phis must be small.
From an experimental point of view the situation is just now becoming intrasting
since the statistical uncertainty of the experimental measuremnt is approaching the Standard
Model prediction. Thus, with the upgraded \lhcb detector \phis enters a high precision era
of measurements, where along with other observables will most likely be able to point towards
a prticular New Physics model.

However, entering this promising high precision era comes along with an important consideration
that has to be taken into acount in order to make a robust claim about the presence of New Physics
in \phis. In particular there are certain subleading effects within the Standard Model, comming from
the so called {\it penguin topologies} and shown in see \figref{app_jpsiphi_tree_peng}, that the
\phis measurement of \equref{app_phis_lhcb} does not take into account. These subleading effects shift
the Standard Model prediction, $\phis^{\tiny \text{SM,tree}}$, by a small amount, $\Delta\phiS{SM,peng}$.
Considering also the fact that, as implied by \equref{app_phis_lhcb_theo}, potential New Physics in
\phis are small: One understands that contributions to \phis from Standard Model subleading penguin
topologies are imposible to disentangle from potential New Physics contributions, which also shift
the Standard Model prediction by, $\Delta\phis^{\tiny \text{NP}}$. The situation is deppicted in
following equation:

\begin{equation}
\centering
 \phis^{\text {eff}} = \phis^{\tiny \text{SM,tree}} + \Delta\phiS{SM,peng} + \Delta\phis^{\tiny \text{NP}}.
 \label{app_phis_sm_peng}
\end{equation}

\noindent Note that increasing the precision of $\phis^{\text {eff}}$ does not help at all in distinguishing
between $\Delta\phiS{SM,peng}$ and $\Delta\phis^{\tiny \text{NP}}$. In addition, the subleading penguin
topologies are supressed with respect to tree in the \BsJpsiPhi decay by two orders of magnitude.
This suppresion factor is reflected to a large undertainty in the estimated penguin topology contributions,
$\Delta\phiS{SM,peng}$, if the \BsJpsiPhi decay channel is used. Thus once again it is not possible to
make a strong claim about New Physics contrubutions to \phis.

Given the above mentioned limitations it becomes mandatory to estimate contributions from
penguing topology to \phis using a different decay chanells, also called control chanells,
\eg the \BsJpsiKst and/or \BdJpsiRho. Note the latter decays have identical tree and penguin
topologies with \BsJpsiPhi. The only difference is that one of the quarks forming the \Pphi
particle in \figref{app_jpsiphi_tree_peng} is replaced with another quark to form the \Kstarz
and \rhoz particles respectivelly for the \BsJpsiKst and \BdJpsiRho decays. This difference in
quarks between the \BsJpsiPhi decay and the control channels introduces uncertainty to
$\Delta\phiS{SM,peng}$, since the impact of assuming perfect symmetry between quarks has to be
estimated and propagated as a systematic uncertainty. However, the penguin topology in both of
the control channels is not suppressed with resect to the tree and thus the uncertainty to
$\Delta\phiS{SM,peng}$ does not suffer from any supression when measured using the
these control channels. Finally, the overal uncertainty of $\Delta\phiS{SM,peng}$ is such
that by using both control channells it is indeed possible to get a handle on potential
New Physics contribution to $\Delta\phis^{\tiny \text{NP}}$.

\begin{figure}[t]
  \begin{subfigure}{0.5\textwidth}
    \raggedright
    {\scalebox{1}{\sffamily \input{Figures/Chapter1/tree}}}
    \label{app_jpsiphi_tree}
  \end{subfigure}%
  \hfill
  \begin{subfigure}{0.5\textwidth}
    \raggedleft
    {\scalebox{1}{\sffamily \input{Figures/Chapter1/penguin}}}
    \label{app_jpsiphi_peng}
  \end{subfigure}
    \caption{Leading order {\it tree} (left) and subleading penguin (right) \BsJpsiPhi decay topologies.}
  \label{app_jpsiphi_tree_peng}
\end{figure}

\subsubsection{Analysing Particle Collisions}
Having introduced the penguin parameter $\Delta\phiS{SM,peng}$ it is intreasting to point out some
aspects releated to the experimental measurement itself and thus highlight the pipeline followed from
the recorded raw particle collisions and the final result.

To begin with the recorded \BsJpsiKst and \BsJpsiRho decays, necessary to extract the penguin paramter,
have to be processed such that the presence of background, noise, is suppressed as much as possible.
Background processes are in general introduced due to finite experimental resolution. Having a clean
sample is always desired; particularly in the case of the \BsJpsiKst decay which has a low probability
of occuring with respect to the \BdJpsiKst decay, see \figref{app_mass_plot}. Furthermore, the spin
configurations of the particles involved in the \BsJpsiPhi decay, where \Jpsimumu and \phiKK, is such
that the direction of the final state particles, mumu and KK, becoems relevant and have to be accounted
for by a multi dimensional model.

\begin{figure}[t]
  \centering
  \tikzsetnextfilename{mass_plot}
  \scalebox{0.5}{\input{Figures/Chapter4/mass_plot_simul_log}}
  \caption{Invariant mass fit to the data, shown with black points. The green and red curves correspond to the \BdJpsiKst and
           \BsJpsiKst mass \pdf components. The combinatorial and \LbJpsippi background components are shown with black and cyan
           colors respectively. The total mass \pdf, blue curve, is overlayed on top of the data. Figure from \cite{bsjpsikst-paper}. }
  \label{app_mass_plot}
\end{figure}

At that point it is usefull to emphasize the fact that in particle physics, parameters of intereset
require a sufficiently large number of recorded particle collisions, data, in order to be estimated.
This welcomes a statistical aproach to aprameter estiamtion where, the larger the data size is the
more precise the estimate on the paramter of intereset becomes. The same holds for a sample size that
is as free of background, noise, as possible. Furthermore, finite experimental resolution as well as
varius the detector responce could introduce biases and have to be taken into account by the previous
multi dimensional model.

Once the data have been cleaned from the presence of background and all the experimental effects are
properly taken into account the model, which depends on the paramter of itnereset, is fitted to the data.
The fitting process is based on the principle of maximum likelihood principle. The likelihood is a
function of the parameters of interest given the observed data. Going one step further each term of
the product is the probability of measuring a certain value for a particular observable, like the
\mass{\Bs} of \figref{app_mass_plot}, given a specific value of the parameter of interest. It follows that
At its maximum the likelihood provides the so called {\it best fit} estimate for the paramters
of interest given the data that was used to build the previous likelihood function.

\subsubsection{Result, Impact and Conclusion}

Having performed the likelihood fit of the multidimensional model to the data and
following \cite{Fleischer:1999zi,Faller:2008gt,DeBruyn:2014oga,DeBruyn-thesis} the
final result for the paramters of intereset are:

\begin{subequations}
\label{app_delta_phis_result}
\begin{align}
\DeltaPhisJpsiPhi{0}         & = 0.000 \; ^{+0.009}_{-0.011} \; {\rm (stat)} \; ^{+0.006}_{-0.007} \; ({\rm syst}) = 0.000 \; ^{+0.010}_{-0.014},\\
\DeltaPhisJpsiPhi{\parallel} & = 0.001 \; ^{+0.010}_{-0.014} \; {\rm (stat)} \; ^{+0.006}_{-0.007} \; ({\rm syst)} = 0.001 \; ^{+0.012}_{-0.016},\\
\DeltaPhisJpsiPhi{\perp}     & = 0.003 \; ^{+0.010}_{-0.014} \; {\rm (stat)} \; ^{+0.006}_{-0.007} \; ({\rm syst)} = 0.003 \; ^{+0.012}_{-0.016},
\end{align}
\end{subequations}

\noindent Note that the parameter $\Delta\phiS{SM,peng}$ is splitted in the so called {\it polarization}
components. Essentially the total \BsJpsiPhi amplitude, or probability, as well as the \phis value
are in principle different depending on the configuration of the particle spins in the \BsJpsiPhi
decay resulting in the three amplitude components $\parallel,\perp,0$. Thus, the penguin parameter
$\Delta\phiS{SM,peng}$ has to be estimated in a similar footing.

The shifts $\DeltaPhisJpsiPhi{k}$ quoted in \equref{app_delta_phis_result}
suggest that contributions of penguin topologies to the \BsJpsiPhi decay amplitude are
small, $<1^\circ$. Given the also small \phis measured value from \lhcb shown in \equref{app_phis_lhcb}
it becomes mandatory to control penguin contributions in future \phis measurements.
Furthermore, the sensitivity from the experimental side is interesting with respect to
the Standard Model prediction, see \equref{app_phis_lhcb_theo}. Thus, potential deviations from this
prediction is going to play a central role in future and more precise \phis measurements.
Increasing the amount of data in the \lhc \runtwo might not be enough to yield
a significant claim on the presence of physics beyond the Standard Model and hence
the upgraded \lhcb detector becomes important in the pursuit for New Physics with \phis in the future.
The result implies that the penguin contribution to the \phis is small, blah.



\begin{itemize}
  \item Fix units in delta phis
  \item use consistent symbols for delta phis in the appendix
\end{itemize}
