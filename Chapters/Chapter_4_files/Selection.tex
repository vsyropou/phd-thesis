
Following the trigger system, addressed in \secref{det_trigger}, there are effectively four stages of offline event
selection which take place after the data has been recorded.
First a set of selection criteria applied is called {\it Stripping}. The purpose of stripping is to roughly
select the triggered data according to their physics content. Furthermore, a loose rejection of the {\it combinatorial}
background is performed such that the data are prepared for the next selection stages.
Combinatorial background is the type of background typical in particle collision where a large number of
particles is produced. It is caused by random combinations of non-signal particles that build a signal
{\it candidate} during the event reconstruction, mentioned in \secref{det_tracking}.
Stripping is also a way to save disk space when permanently storing events offline.
Each of the stripping selection criteria, also referred to as a {\it cut},
requires an observable to satisfy some condition. For example, the mass of the \Bs candidate must be within
a certain range. Or another type of cut could be the probability that a final state particle, like the pion, is misidentified
as a muon. A detailed table of the Stripping selection is given in \tabref{Bs2JpsiKstSelection}.
The remaining three selection steps after Stripping involve a multivariate based selection, \secref{Multivariate_Based_Selection},
further reduction of the combinatorial background, \secref{sWeighting_and_mass} and specific background removal,
\secref{peaking_backgrounds}.

Prior to addressing the various selection steps involved in detail, it is useful to introduce the
\sPlot technique, which is used to statistically subtract the combinatorial background further after the first
candidate selection step of \secref{Multivariate_Based_Selection}. The latter technique is a statistical
tool to unfold data distributions \cite{splot}, for example signal and background. A {\it Probability Density Function},
\pdf, that is able to estimate the contribution-yield of signal and background, in that case, is required as an input.
The \sPlot technique uses information from the likelihood, built from the previous \pdf, to assign a
weight to each candidate. The weights are also called {\sWeights} and are computed such that the contribution
of the background component in data is effectively canceled. Thus a large \sWeight value implies
higher signal probability, for the particular candidate. In \secref{sWeighting_and_mass} the \pdf
used in the current analysis is described and a bit more details on the \sPlot technique are provided.

\subsection{Multivariate Based Selection}
\label{Multivariate_Based_Selection}

The \BsJpsiKst signal yield out of the $3 \invfb$ data-set is expected to be small, see \secref{BspsiKst_at_lhcb}. Thus, one would like to
reject as much background as possible while keeping most of the signal. One way to do that would be to tighten the Stripping
selection criteria one by one. Instead, a multivariate approach, hereafter MVA, is adopted.
In that case a set of observables is combined by an MVA algorithm to produce a single variable, the {\it classifier variable}.
The MVA approach can exploit the correlations between input variables to get the maximum discriminating power out of their combination.
The classifying variable is constructed such that it ranges from $-1$ to $+1$. Signal candidates tend to accumulate near $+1$,
whereas background candidates accumulate near $-1$, see \figref{BTDG_performance}.

In the current analysis the MVA algorithm is a {\it Boosted Decision Tree}, BDT in short, which is a class
of machine learning  algorithm. Further information on the BDT algorithm used can be found in Section 7.2 of \cite{TMVA}.
For the current analysis the TMVA toolkit \cite{TMVA} is used for the MVA selection. In order to construct the classifying variable,
a pair of data sets is needed. One data set must be a representative sample of the signal and one of the background.
This pair of data sets is provided as input to the MVA algorithm to construct the classifying variable.
In that step the MVA algorithm is {\it trained} to distinguish between the signal and background input data sets.
After the training step is completed a second, independent, pair of sets is used to asses how
successful the training step was. This step is called {\it testing}.
For the signal representative samples, \BsJpsiKst Monte-Carlo (MC) simulated data are used, hereafter simulated data.
For the background representative sample data candidates are used. These candidates are taken from a control
region away from the \BsJpsiKst mass peak, usually referred to as {\it mass side-band}. This way the background representative sample
is not affected by non-perfect simulation and it is also independent of the signal representative sample.
Note that the simulated data are treated by all the processing steps after the \lzero trigger level in the same way as data.

\begin{figure}[!t]
\centering
  \tikzsetnextfilename{bdtg}
  \scalebox{1}{\input{Figures/Chapter4/bdtg}}
  \caption{Data BDT distribution (black points). Signal (blue points) and background (red points) distributions
           come from simulated data and the \Bs mass side-band control region, see text for details.}
  \label{BTDG_performance}
\end{figure}

The two well-separated peaks of the data BDT distribution, indicated with black points in \figref{BTDG_performance}, suggest
that the BDT variable achieves a good discrimination between signal and background candidates.
The BDT was also checked for possible over-training. The latter is a situation where the BDT variable becomes sensitive
to statistical fluctuations of the training samples. This means that statistically compatible distributions, such as
the testing and training samples, appear to have significantly different BDT distributions. This is not a desirable
situation which leads to bad BDT performance. Once the training and testing steps are complete, a cutoff value on the
BDT is applied so that it maximizes the following figure of merit, hereafter FoM:

\begin{equation}
  \centering
  F(w_i) = \frac{\left(\sum{w_{i}}\right)^2}{\sum{w_{i}^2}},
\label{eqn:fom}
\end{equation}

\noindent where $w_i$ are weights associated to each candidate and computed with the \sPlot technique,
mentioned earlier in the introduction of the current section. The above-mentioned FoM increases in value with
the sum of \sWeights, in the numerator. Large values of \sWeights translate to higher effective signal yield.
This is because candidates that are more likely to be signal receive a larger weight $w_i$. It follows that the
optimum set of weights will have the higher FoM value compared to any other set of weights. Thus the FoM
of \equref{eqn:fom} essentially expresses the signal yield. For a range of BDT values the \sPlot technique
provides a different set of \sWeights based on which a different FoM value can be obtained, thus allowing for
optimizing the BDT cutoff value.

% ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
\subsection{Reflection Backgrounds}
\label{peaking_backgrounds}

After applying the BDT cut there is still some combinatorial background remaining, see \figref{mass_BDTG_selection},
which is statistically subtracted further by means of the \sWeights in \secref{sWeighting_and_mass}.
However, there is one more crucial step that is necessary to address prior to the application of the
\sPlot technique, which is the treatment of {\it reflection} backgrounds. This type of background is due to particle misidentification.
Specifically a final state particle is assigned the wrong mass hypothesis. As a result the invariant mass of the \Bs candidate,
is shifted with respect to \Bs candidates where all the final state particles were correctly identified.
Studies of simulated samples show contributions
from several specific background sources, such as \BsJpsiKK, \BsJpsipipi and \BdJpsipipi.
These backgrounds are the result of misidentifying a final state particle, for example a pion can
be confused for a kaon. The invariant mass distribution of the above-mentioned misidentified backgrounds
accumulate near the \BJpsiKpi signal peaks. Furthermore given the low number of expected reflection background,
see \tabref{peaking_bkg_yields}, their distribution is quite not smooth and continuous. This type of background is
removed before the next selection step is applied as it is more effective when the distributions involved are smooth.
Therefore, specific simulated decays with negative weights
are injected to the data sample such that they cancel out the contribution of the above-mentioned reflection
background. The term ''cancel out'' obtains its exact meaning in the context of the likelihood which is
built in \secref{sWeighting_and_mass} based on the mass probability density function.
The current section briefly addresses the treatment of the reflection background with negative weights
as well as the special treatment of the \LbJpsippi reflection background.

\begin{figure}[!t]
  \centering
  \tikzsetnextfilename{mass_before_after_bdtg}
  \scalebox{1}{\input{Figures/Chapter4/mass_before_after_bdtg}}
  \caption{Mass distribution after stripping selection (black points), before (blue points) and after (red points) BDT selection.
           The peaks around 5300 and 5360 \mevcc correspond to the \BdJpsiKst and \BsJpsiKst decay channels respectively.}
  \label{mass_BDTG_selection}
\end{figure}

As a first step towards removing the above-mentioned type of background contribution it is necessary to have an estimate
on the expected reflection background yield on data. This is done using simulated data and based on the expression:

\begin{equation}
N_{\rm exp} = 2 \times \sigma_{b\bar{b}}\times f_\quark \times \BF \times \varepsilon \times {\mathcal{L}},
\end{equation}

\noindent where $\sigma_{b\bar{b}}$ is the cross section for the production of a pair of bottom quarks, $f_\quark$ is the hadronization fraction
(probability that the $b$ quark forms a $\Bq$ meson, where $\quark=\dquark,\squark$), $\varepsilon$ is the total efficiency (including reconstruction,
selection and trigger) and ${\mathcal{L}}$ is the integrated luminosity of the data. Finally \BF stands for the branching fraction
of the particular background of \tabref{peaking_bkg_yields}. Since simulated data are used it is necessary to determine
the effective integrated luminosity in the simulated sample and adjust it such that it matches the estimated integrated
luminosity of the data. After that, the fraction of reflection background from the simulated sample is a valid estimate
of the reflection background fraction in data (assuming same total efficiency between the simulation and data sample).

The second and last step of the reflection background removal is to apply an angular correction factor to account for
the fact that simulated decays are distributed uniformly in the $(\Omega, \mkpi)$ phase space and hence do not
represent a proper decay amplitude structure. This can cause the reflection background yield estimations to be
imperfect because the simulated events are distributed in the previous phase space in a different way than the
actual reflection background in the data. The symbol $\Omega$ represents the angles that describe the \BsJpsiKst
decay, see \secref{Diferential_Decay_Rate} for further details. The amplitude analyses of \BdJpsipipi \cite{SheldonBdpipi},
\BsJpsipipi \cite{SheldonBspipi}, \BsJpsiKK \cite{SheldonKK} and \LbJpsipK \cite{Gao:1701984} provide the necessary
knowledge to compute the correction factors:

\begin{equation}
\centering
w_{\rm MC}^i = \frac{P_{\rm DATA}^i (\Omega, \mhh  | A_k^i)} {P_{\rm MC}^i(\Omega, \mhh)},
\end{equation}

\noindent where $P_{\rm DATA}$ and $P_{\rm MC}$ are normalized \pdfs.
The index $i$ labels the species of the particular reflection background, \ie \BdJpsipipi, \BsJpsipipi, \BsJpsiKK, \LbJpsipK.
The polarization states of each of the above species is labeled by the $k=\left(0, \parallel, \perp, S, D \right)$ index.
Where the first three indices represent the \pwave and the last two the \swave and \dwave respectively,
see \secref{Diferential_Decay_Rate} for more details. The estimated yields of reflection background after
the above-mentioned steps are shown in \tabref{peaking_bkg_yields}

\begin{table}[t]
   \centering
        \begin{tabular}{c c c}
          \hline
          \multicolumn{2}{c}{Sample} & $\pm70\mevcc$ window \\
          \hline
          \multirow{ 2}{*}{\BdJpsipipi} & 2011 & $51 \pm 10$ \\
                                        & 2012 & $115\pm 23$ \\
          \hline
          \multirow{ 2}{*}{\BsJpsipipi} & 2011 & $9.3\pm 2.1$ \\
                                        & 2012 & $25.0\pm 5.4$\\
          \hline
          \multirow{ 2}{*}{\BsJpsiKK}   & 2011 & $10.1 \pm 2.3$ \\
                                        & 2012 & $19.2 \pm 4.0$ \\
          \hline
          \multirow{ 2}{*}{\LbJpsipK}   & 2011 & $36 \pm 17$ \\
                                        & 2012 & $90 \pm 43$ \\
          \hline
          \multirow{ 2}{*}{\LbJpsippi}  & 2011 & $13.8 \pm 5.3$ \\
                                        & 2012 & $27.3 \pm 9.0$ \\
        \hline
        \end{tabular}
        \caption{Estimated reflection yields in a $\pm 70\mevcc$ $\mkpi$ window of each background
        specie after weighting for the angular distributions, with the exception of the \LbJpsippi as explained
        at the end of \secref{peaking_backgrounds}.}
        \label{peaking_bkg_yields}
\end{table}

As it was previously mentioned, the \LbJpsippi background is treated differently. Instead of following the above
procedure, the \LbJpsippi invariant mass line shape is modeled and statistically subtracted in the same way as
the combinatorial background in the next section. The reason for this different treatment with respect to other
reflection backgrounds is due the \LbJpsippi amplitude structure \cite{Aaij:2014zoa}, which was not sufficiently
known when the analysis was concluded. Thus weighting of the simulated samples was not possible.
Now since shape of the misidentified \LbJpsippi decays in the $\jpsi\kaon\pion$ mass spectrum is broad
enough, see \figref{mass_plot}, and the \sPlot technique can be used to subtract this specific reflection
background.

% ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
\subsection{sWeighting and Invariant Mass Distribution}
\label{sWeighting_and_mass}

After injecting candidates with negative weights to cancel reflection background contributions,
the data sample is effectively composed of, \BdJpsiKpi, \BsJpsiKpi, \LbJpsippi, and combinatorial background not removed by the
multivariate based selection. These four modes are statistically disentangled using the \sPlot technique \cite{splot}.
The technique requires as an input a \pdf for each of the above species. The \pdf has to be {\it extended},
which means that it is possible to estimate the yield of each species by means of an extended maximum likelihood fit.
The current analysis uses the $\jpsi \kaon\pion$ invariant mass \pdf, hereafter just mass, to perform the extended maximum
likelihood fit to the mass distribution of data, hereafter mass fit. It is important to point out that the \sPlot
technique assumes no correlation between the mass and other variables.
A systematic uncertainty corresponding to the validity of this assumption will be assigned later.

\subsubsection{The mass \pdf}
Before discussing the mass fit description it is useful to briefly describe the \pdfs used for each of the following individual species:

\begin{itemize}
\item Combinatorial background: Exponential distribution.
\item \LbJpsippi decays: Amoroso distribution \cite{Amoroso}
\item \Bd and \Bs signals: Hypatia distribution \cite{Santos:2013gra}.
\end{itemize}

\noindent An exponential distribution, $e^{-km}$, is found to be an appropriate description of the combinatorial background. It accounts
for cases where random combinations of final state particles are combined to a signal candidate. Such candidates are not expected to
exhibit any structure in the mass distribution, they simply follow an exponential distribution with a negative slope.
The negative slope can be understood when considering two main effects. First, low invariant mass implies a low momentum of
the particles (\kaon, \pion, \mmu) used to reconstruct the \BsJpsiKst candidate. Second, there are more low momentum
particles produced in a $\proton-\proton$ collision compared to high momentum ones. This is because elastic-soft strong
interactions between colliding {\it partons}, quarks and gluons contained inside the proton, are more likely to happen
compared to central, head-on, ones.

As for the \LbJpsippi background, the Amoroso distribution was found to provide  good description of the data.
The parameters of the distribution are obtained from simulated data and then fixed in the mass fit to the data. The first two parameters
are the mean and width of the distribution, whereas the other two are related to the shape. The Amoroso distribution describes
an entire family of distributions: Depending on the values of the shape parameters the resulting distribution can vary from exponential
to a Gaussian distribution. This is a powerful feature that is exploited in order to describe the \LbJpsippi reflection background shape.

The Hypatia distribution has two main features which makes it a suitable description of \Bs and \Bd signal invariant mass distributions.
First, a hyperbolic core is found to describe the peak of the previous distributions adequately. Second, invariant mass resolution
effects are taken into account by marginalizing over an approximate $\mJpsiKpi$ mass uncertainty distribution. Overall the Hypatia function adequately
models the tails of the \Bs and \Bd invariant mass distribution which is of crucial importance. This is because the previous invariant
mass distributions are sufficiently close, see \figref{mass_plot}, such that bad modeling of the tails implies that a certain number of \Bd decays
are associated to \Bs and \viceversa. Several effects contribute to the structure of the tails apart
from mass resolution of the detector. These effects can be radiative tails (a charged final state particle radiates a photon), interplay of
radiative tail with \jpsi mass constraint (applied when building the $\Bs$ candidate) or badly reconstructed candidates caused by decays of the final state hadrons, see \cite{Santos:2013gra}.
The tail parameters ($\alpha$, $n$) are four in total (two for each tail) and are taken from a fit to MC simulated events with a known resolution.
This makes sure that the tail parameters do not rely on detector simulation imperfections.
For the core of invariant mass distribution, Hypatia requires five shape parameters, namely $\zeta$, $\beta$, $\lambda$, $\sigma$, $\mu$.
The first two are set to zero since $\zeta$ is empirically found to be very small whereas $\beta = 0$ implies that the core is
symmetric left and right with respect to the mean. The third parameter is taken from the previous simulated sample along
with the tail parameters. The latter, in the limit of $\zeta = 0$, $\lambda$ does not depend on detector effects but only
on particle kinematics, the same way as the tail parameters do. Lastly, the remaining two parameters, the width and
the mean of the core, are determined by the mass fit to the data.

\subsubsection{The mass fit}
From studies with simulated data, some of the \Bs and \Bd Hypatia parameters appear to be significantly correlated with the \mkpi
invariant mass. Since these parameters need to be fixed in the mass fit, the latter is performed in intervals of the
\mkpi invariant mass. In addition, due to correlations between the mass and one of the variables, $\cos\thetamu$, used in
the subsequent angular fit, the requirements of the \sPlot technique are not satisfied, see \secref{sWeighting_and_mass},
and thus it cannot be applied directly.

Therefore, each \mkpi invariant mass sub-sample is divided further in intervals of $\cos\thetamu$ where the \sPlot technique
can be applied. The final fit to the data is shown in \figref{mass_plot}, whereas the overall \Bs and \Bd yields are:

\begin{align}
  \centering
  N_{\Bd} &= 208656  \pm  462 ^{+ 78}_{-76}, \\
  N_{\Bs} &= \phantom{00}1808  \pm  51 ^{+ 38}_{-33} \, ,
  \label{signal_yields}
\end{align}

\noindent where the first uncertainties are statistical and obtained from the quadratic sum of the ones in each fitting category,
and the second are systematical uncertainties. The correlations between the \Bd and \Bs yields in each fitting category
are found to be less than $4\%$. Note that the mass fitting model has been validated with a procedure commonly called {\it pseudo-experiments}.
This procedure checks if the fitting model introduces a bias to the parameters of interest. Further details can be found in
\secref{Toy_Experiments_Study} where a pseudo-experiment study is performed for the angular fitting model of \secref{Total_Decay_Rate}.

\subsubsection{sWeighting}
After the mass fit is performed, all the necessary ingredients to remove the remaining background are in place.
The fitted mass \pdf is now given to the \sPlot algorithm. The algorithm assigns a weight to each candidate
based on the likelihood function built from the input \pdf. The main idea of \sPlot is that for each of the
species of the total \pdf a set of weights can be computed such that they project out only that particular
species while effectively subtracting the others. Based on this technique, \BsJpsiKst candidates are selected
out of the full data sample.

\begin{figure}[t]
  \centering
  \tikzsetnextfilename{mass_plot}
  \scalebox{0.5}{\input{Figures/Chapter4/mass_plot_simul_log}}
  \caption{Invariant mass fit to the data, shown with black points. The green and red curves correspond to the \BdJpsiKpi and
           \BsJpsiKpi mass \pdf components. The combinatorial and \LbJpsippi background components are shown with black and cyan
           colors respectively. The total mass \pdf, blue curve, is overlaid on top of the data. Figure from \cite{bsjpsikst-paper}. }
  \label{mass_plot}
\end{figure}

The advantage of the \sWeights approach becomes apparent when performing the angular analysis, described in the subsequent section.
Specifically, the resulting weighted candidates can be described by the signal \pdf only, meaning there is no need to explicitly model
the background angular distributions. As a result the fit itself is faster and simpler to implement. On the other hand there are a few
disadvantages such as the evaluation of systematic uncertainties on the estimated angular parameters due to the mass \pdf modeling:
Any variation of these models implies re-computing the \sWeights. Also, care must be taken to ensure that the uncertainties
of the fitted angular parameters are properly estimated \cite{splot}. The uncertainties tend to be underestimated in a fit
with weighted candidates. This is because the per candidate weighs in a weighted likelihood fit change the shape of the
likelihood function, which is where the statistical uncertainty estimation comes from.
Thus to correct for this distortion of the likelihood function shape the scale factor:

\begin{equation}
  \centering
  \alpha = \left(\frac{\sum_{i} w_i}{\sum_{i} w_i^2}\right)^{1/2},
  \label{sWeights_scale_factor}
\end{equation}

\noindent is applied to each weight before performing the angular fit. The scale factor essentially ensures that the sum of weights
equals the number of candidates. A detailed derivation of $\alpha$ can be found in \cite{jeroenThesis}.
