
Effectively there are four stages of offline event selection which take place after the data has been recorded.
First a set of selection criteria is applied called {\it Stripping}. The purpose of stripping is to perform
a loose rejection of the {\it combinatorial} background and prepare the data for the next stages of selection.
Combinatorial background is the type of background typical in particle colision with a large number of secondary
particles produed. It is casued by random combinations of non-signal partices that build a signal candidate during
the event reconstruction. For more details on the last see \secref{reconstruction},
Stripping is also a way to save disk space for offline event storage, since it is impossible to save anything
that the detector writes out. Each of the stripping selection criteria, also referred to as a {\it cut},
requires from a certain observable to satisfy some condition. For example the mass of the mother particle \Bs must be within
a certain range. Or another type of cut could be the probability that a final state particle, like the pion, is misidentified
as a muon. A detailed table of the Stripping selection is given in \tabref{Bs2JpsiKstSelection}. The remaining three selection
steps after Stripping are described in each of the subsequent subsections.

\subsection{Multivariate Based Selection}
\label{Multivariate_Based_Selection}

The \BsJpsiKst signal yield out of the full 3 \invfb data-set is expected to be small, see \secref{BspsiKst_at_lhcb}. Thus, one would like to
reject as much background as possible while keeping all the signal. One way to do that would be to tighten the Stripping selection criteria
one by one\footnote{also known as {\it cut-based analysis}}. Instead, a multivariate approach (here after {\it MVA}) is adopted.
In that case a set of variables are combined by the MVA algorithm to produce a single variable, the {\it classifier variable}.
The MVA approach can exploit the correlations between input variables to get the maximum discriminating power out of their combination.
The classifying variable is constructed such that it ranges from $-1$ to $+1$. Signal candidates tend to accummulate near $+1$,
whereas background candidates near to -1. In the current analysis the calsifyer variable is aslo refered to as BDT, which states
the class of MVA algorithm implemented, namelly {\it Boosted Decision Tree}. For further information on these types of algorithms
can be found in {\color{red}Ref TMVA toolkit and some paper on boosted decision trees maybeeeee.......}

\begin{figure}[!h]
\begin{center}
  \tikzsetnextfilename{bdtg}
  \scalebox{1}{\input{Figures/Chapter4/bdtg}}
  \caption{BDT distribution on real data. $y$-axis in logarithmic scale. Signal (Background) candidates accumulate mainly towards left the right (left). }
  \label{BTDG_performance}
\end{center}
\end{figure}

For the current analysis the TMVA toolkit~\cite{TMVA} is used for the MVA selection. In order to construct the classifying variable,
a pair of data sets is needed. One data set must be a representative sample of the signal and one of the background. {\color{red} These sampels are shown with blah color in \figref{BTDG_performance}}
This pair of data sets is provided as input to the MVA algorithm to construct the classifying variable.
In that step the MVA algorithm is {\it trained} to distinguish between the signal
and background input data sets. Next another, independent, pair of sets is used to asses how successful the training step was. This step is called {\it testing}.
For the signal representative samples, \BsJpsiKst Monte-Carlo simulated data (here after simulated data)
are used. As for the background representative sample, real data candidates are used. These candidates are taken from a control
region, usually refered to as {\it mass sideband}, away from the \BsJpsiKst mass peak. This way the background representative sample
is not affected by non perfect simulation and it is also indipendant of the signal representative one.
Note that the simulated data are treated in the same way as the real data do by all the processing steps after the \lone trigger level.

The two well seperated peaks in \figref{BTDG_performance} suggest that the discriminating variable, BDT, shows a good discrimination
power over signal and background distributions \figref{BTDG_performance}.
The BDT was checked for possible over-training. The last is a situation that the BDT becomes sensitive to statistical fluctuations
of the training samples. Theis means that statisically compatible distribution, such as the testing and training samples,
apear to have significantly different BDT distrinbutions. Which is a not desireable situation that leads to bad BDT performance.
Once the training and testing steps are complete, a cutoff value on the BDT is applied so that it maximizes the following figure of merit
(hereafter {\it FoM})~\cite{Yuehong_fom}:

\begin{equation}
  \centering
  F(w_i) = \frac{\left(\sum{w_{i}}\right)^2}{\sum{w_{i}^2}},
\label{eqn:fom}
\end{equation}

\noindent where $w_i$ are weights associated to each event (here after \sWeights), and calculated with the \sPlot
technique. The last  is a statistical tool to unfold data distributions~\cite{splot}. For example disentangle signal from background.
Further details can be found in \secref{sWeighting_and_mass}. The FoM of \equref{eqn:fom} can be understood as an effective signal yield.
For a range of BDT values the \sPlot technique provides a different set of \sWeights based on which a different FoM value can be obtained.
The optimum BDT value is chosen as the one that maximizes $F(w_i)$.

% ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
\subsection{Non-combinatorial Backgrounds}
\label{peaking_backgrounds}

After applying the BTD cut there is still some combinatorial background remaining, see \figref{mass_BDTG_selection}, which is removed
further by means of the \sWeights described in \secref{sWeighting_and_mass}. However, there is
one more crucial step that is necessary to address prior to the application of the \sPlot technique,
namely that of non-combinatorial background.
Studies of simulated samples show contributions from several specific background sources, such as \BsJpsiKK, \BsJpsipipi and \BdJpsipipi.

\begin{figure}[!h]
  \begin{center}
  \tikzsetnextfilename{mass_before_after_bdtg}
  \scalebox{1}{\input{Figures/Chapter4/mass_before_after_bdtg}}
  \caption{Mass distribution before (black) and after (red) BDT selection. $y$-axis in logarithmic scale}
  \label{mass_BDTG_selection}
  \end{center}
\end{figure}

\noindent Those backgrounds are the result of misidentifying a final state particle, for example a pion can
be confused for a kaon. The invariant mass distribution of the above mentioned misidentified backgrounds
accumulate near the \BJpsiKpi signal peaks. Furthermore given the low number of expected non-combinatorial background events,
see \tabref{peaking_bkg_yields}, their distribution is quite non-uniform and discontinues. This type of distribution has to be removed
before the next selection step described in \secref{sWeighting_and_mass} can be applied as it is more effective
when the distributions involved are smooth. Therefore, specific simulated events with negative weights
are injected to the data sample such that they cancel out the contribution of the above mentioned non-combinatorial background
events\footnote{The term "cancel out" obtains more meaning in the context of the likelihood function which is built in
the next selection step \secref{sWeighting_and_mass} based on a given mass \pdf. }.
The current section briefly addresses the treatment of the non-combinatorial background with negative weights
as well as the special treatment of the \LbJpsippi non-combinatorial background.

As a first step towards removing the above mentioned type of background contribution it is necessary to have an estimate
on the expected non-combinatorial background yield on data. This is done using simulated data and based on the expression:
\begin{equation}
N_{\rm exp} = 2 \times \sigma_{b\bar{b}}\times f_q \times BR \times \varepsilon \times {\mathcal{L}},
\end{equation}
\noindent where $\sigma_{b\bar{b}}$ is the cross section for the production of a pair of bottom quarks, $f_q$ is the hadronization fraction
(probability that the $b$ quark forms a meson with another quark type $q$), $\varepsilon$ is the total efficiency including reconstruction,
selection and trigger) and ${\mathcal{L}}$ is the integrated luminosity, hearafter just luminosity, of the data. Finally BR stands for the
branching fraction of the particular background of \tabref{peaking_bkg_yields}. Since simulated data are used it is necessary to determine
the effective luminosity of that simulated sample and adust it such that it muches the estimated luminosity of the data. After that, the
number of non-combinatorial background events from the simulated sample is a valid estimate of the one in real
data\footnote{Assuming same total efficiecny between the simulationa and real data sample.}.

The second and last step of the non-combinatorial background removal is to apply an angular correction factor to account for the fact that
simulated events are distributed uniformly in phase space and hence do not represent a proper decay amplitude stracture.
This can cause the non-combinatorial background yield estimations to be wrong because the simulated events are distributed in the $(\Omega, m_{K\pi})$ space
in a different way than the actual non-combinatorial background in the data. The amplitude analysis of \BdJpsipipi, \BsJpsipipi, \BsJpsiKK and \LbJpsipK
which has been performed in~\cite{SheldonBdpipi},~\cite{SheldonBspipi},~\cite{SheldonKK} and~\cite{Gao:1701984} respectively provides

\begin{equation}
\centering
w_{\rm MC}^i = \frac{P_{\rm DATA}^i (\Omega, m_{hh}  | A_k^i)} {P_{\rm MC}^i(\Omega, m_{hh})},
\end{equation}

\noindent where $P_{\rm DATA}$ and $P_{\rm MC}$ are normalized \pdfs\footnote{\pdf is an acronym for Probability Density Function}.
The index $i$ labels the specie of the particuar non-combinatorial background, meaning \BdJpsipipi, \BsJpsipipi, \BsJpsiKK, \LbJpsipK.
Whreas the polarization states, $0$, $\parallel$, $\perp$, of each of the above mentioned species is represented by the $k$ index.

\begin{table}[!h]
   \begin{center}
        \begin{tabular}{c c c}
          \hline
          \multicolumn{2}{c}{Sample} & $\pm70\mevcc$ window \\
          \hline
          \multirow{ 2}{*}{\BdJpsipipi} & 2011 & $51 \pm 10$ \\
                                        & 2012 & $115\pm 23$ \\
          \hline
          \multirow{ 2}{*}{\BsJpsipipi} & 2011 & $9.3\pm 2.1$ \\
                                        & 2012 & $25.0\pm 5.4$\\
          \hline
          \multirow{ 2}{*}{\BsJpsiKK}   & 2011 & $10.1 \pm 2.3$ \\
                                        & 2012 & $19.2 \pm 4.0$ \\
          \hline
          \multirow{ 2}{*}{\LbJpsipK}   & 2011 & $36 \pm 17$ \\
                                        & 2012 & $90 \pm 43$ \\
          \hline
          \multirow{ 2}{*}{\LbJpsippi}  & 2011 & $13.8 \pm 5.3$ \\
                                        & 2012 & $27.3 \pm 9.0$ \\
        \hline
        \end{tabular}
        \caption{Approximated expected yields in $\pm 70\mevcc$ $K\pi$ mass window of each background after re-weighting
                 the angular distributions, with the exception of the \LbJpsippi for which the amplitude structure is not yet deterimened.}
        \label{peaking_bkg_yields}
   \end{center}
\end{table}

\noindent The role of the mplitude polarization becomes more clear in \secref{Angular_Analysis}.
The final non-combinatorial background yields estimation after the above mentioned steps is shown in \tabref{non-combinatorial_bkg_yields}

As it was previously mentioned, the \LbJpsippi background is treated specially. Instead of following the above
procedure, the \LbJpsippi invariant mass line shape is modeled and statistically subtracted in the same way as the
combinatorial background in the next section. The reasons for this different treatment with respect to other
non-combinatorial backgrounds are twofold:
\begin{itemize}
\item The full amplitude structure of \LbJpsippi decays is not yet sufficiently known, and thus the weighting of the simulated samples is not possible.
\item The peak of the misidentified \LbJpsippi decays in the \jpsi$K\pi$ mass spectrum is broader than those of the other
      non-combinatorial backgrounds see \figref{mass_plot}, as a result the \sPlot technique is still effective.
\end{itemize}

% ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
\subsection{sWeighting and Invariant Mass Distribution}
\label{sWeighting_and_mass}

After successfully inecting simulated events with negative weights to cancel non-combinatorial background contributions,
the data sample is effectivelly composed of, \BdJpsiKpi, \BsJpsiKpi, \LbJpsippi, and combinatorial background not removed by the
multivariate based selection. These four modes (also refereed to as species) are statistically
disentangled using the \sPlot technique~\cite{splot}. The technique requires as an input a \pdf that estimates the yield of each
of the above species. The current analysis uses the \pdf of the $\jpsi K\pi$ invariant mass, hereafter just mass, where the yield of each specie
is estimated with an extended maximum likelihood fit to the mass distribution on data, hereafter mass fit.

\subsubsection{The mass \pdf}
Before discussing the mass fit description it is useful to briefly describe the \pdfs used for each of the individual species, which are the following:

\begin{itemize}
\item Combinatorial background: Exponential distribution.
\item \LbJpsippi decays: Amoroso distribution~\cite{Amoroso}
\item \Bd and \Bs signals: Hypatia distribution~\cite{Santos:2013gra}.
\end{itemize}

\noindent An exponential distribution, $e^{-km}$, is found to be an appropriate description of the combinatorial background. It  accounts
for events where random combinations of final state particles are combined to a signal candidate. Those kind of events are not expected to
exhibit any structure in the mass distribution, they simply follow a exponential distribution with a negative slope in the mass spectrum.
The slope is negative because with increasing mass the frequency of those random combinations drops. Since high momentum particles are more
likely to orginate from a singal than low momentum ones. This is beacuse the singal patciles liek the \Bs messon is quite heavy which is
depicted in high momentum of the secondary particles that it decays into.

As for the \LbJpsippi background, the Amoroso distribution was found to provide good description of the data.
The parameters of the distribution are obtained from simulated data and then fixed in the mass fit to the data. The first two parameters
are the mean and width of the distribution, whereas the other two are related to the shape. Actually the Amoroso distribution describes
an entire family of distributions. Depending on the values of the shape parameters the resulting distribution can vary from an exponential
to Gaussian distribution. This is a powerful feature that is exploited in order to describe the \LbJpsippi non-combinatorial background shape.

The Hypatia distribution is chosen mainly because it describes the tails of the \Bs and \Bd invariant mass peaks which is
of crucial importance. These two peaks are sufficiently close,
see picture \figref{mass_plot}, that, in case of bad modeling of the tails, event leakage between the \Bs and \Bd peaks
takes place. Several effects contribute to the structure of the tails apart from mass resolution of the detector. These effects
can be radiative tails (a charged final state particle radiates a photon), interplay of radiative tail with \jpsi mass constraint
or badly reconstructed events caused by decays of the final state hadrons, see~\cite{Santos:2013gra}. The tail parameters ($\alpha$, $n$)
are four in total (two for each tail) and are taken from a fit to MC simulated events with a known resolution. The last makes sure that
the tail parameters do not rely on detector simulation imperfections.
For the core of invariant mass distribution, Hypatia uses five shape parameters, namely $\zeta$, $\beta$, $\lambda$, $\sigma$, $\mu$.
The first two are set to zero\footnote{$\zeta$ is empirically found to be very small whereas
$\beta = 0$ implies that the core is symmetric left and right with respect to the mean.}, the third one is taken from the previous
MC simulated sample along with the tail parameters \footnote{In the limit of $\zeta = 0$ $\lambda$ does not depend on detector
effects but only on particle kinematics, the same way as the tail parameters do.} whereas the last two which are the width and
the mean of the core are estimated by the mass fit.

\subsubsection{The mass fit}
From MC studies, some of the \Bs and \Bd Hypatia parameters appear to be significantly correlated with the \mkpi invariant mass.
Since these parameters need to be fixed in the mass fit, the latter is performed in bins of the
\mkpi invariant mass. In addition, due to correlations between the mass and one of the variables, $\cos\thetamu$, used in the
subsequent angular fit, the requirements of the \sPlot technique are not satisfied and thus it cannot be applied directly.
Therefore, each \mkpi invariant mass sub-sample is divided further in intervals of $\cos\thetamu$ where the \sPlot technique
can be applied.  The mass final mass fit to the data is shown in \figref{mass_plot}, whereas the overall \Bs and \Bd yields are:

\begin{align}
N_{\Bd} &= 208656  \pm  462 ^{+ 78	}_{- 76}\\
N_{\Bs} &= \phantom{00}1808  \pm   51 ^{+ 38	}_{- 33} \, ,
\label{signal_yields}
\end{align}

\noindent where the first uncertainties are statistical and obtained from the quadratic sum of the ones in each fitting category,
and the second uncertainties correspond to systematical. The correlations between the \Bd and \Bs yields in each fitting category
are found to be less than $4\%$. Note that the mass fitting model has been validated with a procedure similar to the
one described in \secref{Toy_Experiments_Study}.

\begin{figure}[!h]
\begin{center}
  \tikzsetnextfilename{mass_plot}
  \scalebox{0.5}{\input{Figures/Chapter4/mass_plot_simul_log}}
  \caption{Invariant mass fit to the data. $y$-axis is in logarithmic scale.}
  \label{mass_plot}
\end{center}
\end{figure}

\subsubsection{sWeighting}
Having performed the mass fit all the necessary ingredients to remove the remaining background are in place.
The fitted mass \pdf is now given to the \sPlot algorithm. The algorithm assigns a weight (also called sWeight), to each event,
based on the likelihood function built from the input \pdf. The main idea of \sPlot is that for each of the species of the total
\pdf a set of weights can be computed such that they project out only that particular species while effectivelly subtract the others.
In the context of the current analysis events that are likely to be background such as combinatorial and \LbJpsipK are
subtracted while \BJpsiKst events are projected with the above mentioned technique (also referred to as sWeighting).

The advantage of the \sWeights approach becomes apparent when performing the angular analysis, described in the subsequent section.
Specifically, the resulting weighted events can be described by the signal \pdf only, meaning there is no need to explicitly model
the background angular distributions. As a result the fit itself is faster and simpler to implement. On the other hand there are a few
disadvantages such as the evaluation of systematic uncertainties on the estimated angular parameters due to the mass \pdf modeling:
Any variation of these models implies re-computing the \sWeights. Also care needs to be taken such that the uncertainties
of the fitted angular parameters are prperly estiamted \cite{splot}. The uncertainties tend to be underestimated in a fit with weighted
events. This is because the per event weights in a weighted likelihood fit change the shape of the likelihood function, and this is
where the statistical uncertainty estimation comes from. Thus to correct for this the scale factor

\begin{equation}
  \centering
\alpha = \left(\frac{\sum_{i} w_i}{\sum_{i} w_i^2}\right)^{1/2}
\label{sWeights_scale_factor}
\end{equation}

\noindent is applied to each weight before performing the angular fit. The scale factor essentially ensures that the sum of weights
equals the  number of events.
