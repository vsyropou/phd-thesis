
Effectively there are four stages of offline event selection which take place after the data has been recorded.
First a set of selection criteria is applied called {\it Stripping}. The purpose of stripping is to perform
a loose rejection of the {\it combinatorial} background and prepare the data for the next selection stages.
Combinatorial background is the type of background typical in particle collision where a large number of
particles is produced. It is caused by random combinations of non-signal particles that build a signal
candidate during the event reconstruction, see \secref{det_tracking}.
Stripping is also a way to save disk space when permanently storing events offline, since it is impossible to save anything
that the detector writes to disk. Each of the stripping selection criteria, also referred to as a {\it cut},
requires from a certain observable to satisfy some condition. For example the mass of the \Bs candidate must be within
a certain range. Or another type of cut could be the probability that a final state particle, like the pion, is misidentified
as a muon. A detailed table of the Stripping selection is given in \tabref{Bs2JpsiKstSelection}. The remaining three selection
steps after Stripping are described in each of the subsequent subsections.

\subsection{Multivariate Based Selection}
\label{Multivariate_Based_Selection}

The \BsJpsiKst signal yield out of the full 3 \invfb data-set is expected to be small, see \secref{BspsiKst_at_lhcb}. Thus, one would like to
reject as much background as possible while keeping all the signal. One way to do that would be to tighten the Stripping selection criteria
one by one\footnote{also known as {\it cut-based analysis}}. Instead, a multivariate approach, here after MVA, is adopted.
In that case a set of variables are combined by the MVA algorithm to produce a single variable, the {\it classifier variable}.
The MVA approach can exploit the correlations between input variables to get the maximum discriminating power out of their combination.
The classifying variable is constructed such that it ranges from $-1$ to $+1$. Signal candidates tend to accumulate near $+1$,
whereas background candidates near -1, see \figref{BTDG_performance}.

In the current analysis the classifier variable is a {\it Boosted Decision Tree}, BDT in short, which is a class
of machine learning  algorithm. Further information on the BDT algorithm used can be found in Section 7.2 of\cite{TMVA}.
For the current analysis the TMVA toolkit\cite{TMVA} is used for the MVA selection. In order to construct the classifying variable,
a pair of data sets is needed. One data set must be a representative sample of the signal and one of the background.
This pair of data sets is provided as input to the MVA algorithm to construct the classifying variable.
In that step the MVA algorithm is {\it trained} to distinguish between the signal and background input data sets.
Next another, independent, pair of sets is used to asses how successful the training step was. This step is called {\it testing}.
For the signal representative samples, \BsJpsiKst Monte-Carlo (MC) simulated data are used, here after simulated data.
For the background representative sam-

\begin{figure}[!t]
\centering
  \tikzsetnextfilename{bdtg}
  \scalebox{1}{\input{Figures/Chapter4/bdtg}}
  \caption{Data BDT distribution (black points). Signal (blue points) and background (red points) distributions
           tend to accumulate mainly to the right and left respectively.}
  \label{BTDG_performance}
\end{figure}

\noindent -ple real data candidates are used. These candidates are taken from a control
region away from the \BsJpsiKst mass peak, usually referred to as {\it mass side-band}. This way the background representative sample
is not affected by non perfect simulation and it is also independent of the signal representative sample.
Note that the simulated data are treated by all the processing steps after the \lzero trigger level in the same way as real data.

The two well-separated peaks in \figref{BTDG_performance} suggest that the BDT variable, shows a good discrimination
power over signal and background distributions. The BDT was also checked for possible over-training. The later is a
situation where the BDT variable becomes sensitive to statistical fluctuations of the training samples.
This means that statistically compatible distributions, such as the testing and training samples,
appear to have significantly different BDT distributions, which is a not desirable situation that leads to
bad BDT performance. Once the training and testing steps are complete, a cutoff value on the BDT is applied
so that it maximizes the following figure of merit, hereafter FoM:

\begin{equation}
  \centering
  F(w_i) = \frac{\left(\sum{w_{i}}\right)^2}{\sum{w_{i}^2}},
\label{eqn:fom}
\end{equation}

\noindent where $w_i$ are weights associated to each event, hereafter \sWeights, and calculated with the
\sPlot technique. The last  is a statistical tool to unfold data distributions\cite{splot}. More details are provided
in \secref{sWeighting_and_mass}. The above-mentioned FoM can be understood as an effective signal yield.
For a range of BDT values the \sPlot technique provides a different set of \sWeights based on which a different
FoM value can be obtained. The optimum BDT value is chosen such that it maximizes $F(w_i)$.

% ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
\subsection{Non-combinatorial Backgrounds}
\label{peaking_backgrounds}

After applying the BTD cut there is still some combinatorial background remaining, see \figref{mass_BDTG_selection}, which is removed
further by means of the \sWeights described in \secref{sWeighting_and_mass}. However, there is
one more crucial step that is necessary to address prior to the application of the \sPlot technique,
namely that of non-combinatorial background. Studies of simulated samples show contributions from
several specific background sources, such as \BsJpsiKK, \BsJpsipipi and \BdJpsipipi.
These backgrounds are the result of misidentifying a final state particle, for example a pion can
be confused for a kaon. The invariant mass distribution of the above mentioned misidentified backgrounds
accumulate near the \BJpsiKpi signal peaks. Furthermore given the low number of expected non-combinatorial background events,
see \tabref{peaking_bkg_yields}, their distribution is quite non-uniform and discontinuous.
This type of distribution has to be removed before the next selection step can be applied as it is more effective
when the distributions involved are smooth. Therefore, specific simulated events with negative weights
are injected to the data sample such that they cancel out the contribution of the above mentioned non-combinatorial background
events\footnote{The term "cancel out" obtains proper meaning in the context of the likelihood which is built
in \secref{sWeighting_and_mass} based on the mass probability density function.}.
The current section briefly addresses the treatment of the non-combinatorial background with negative weights
as well as the special treatment of the \LbJpsippi non-combinatorial background.

\begin{figure}[!t]
  \centering
  \tikzsetnextfilename{mass_before_after_bdtg}
  \scalebox{1}{\input{Figures/Chapter4/mass_before_after_bdtg}}
  \caption{Mass distribution after offline selection (black points), before (blue points) and after (red points) BDT selection.}
  \label{mass_BDTG_selection}
\end{figure}

As a first step towards removing the above mentioned type of background contribution it is necessary to have an estimate
on the expected non-combinatorial background yield on data. This is done using simulated data and based on the expression:
\begin{equation}
N_{\rm exp} = 2 \times \sigma_{b\bar{b}}\times f_q \times \BR \times \varepsilon \times {\mathcal{L}},
\end{equation}
\noindent where $\sigma_{b\bar{b}}$ is the cross section for the production of a pair of bottom quarks, $f_q$ is the hadronization fraction
(probability that the $b$ quark forms a meson with another quark type $q$), $\varepsilon$ is the total efficiency (including reconstruction,
selection and trigger) and ${\mathcal{L}}$ is the integrated luminosity, hereafter just luminosity, of the data. Finally \BR stands for the
branching fraction of the particular background of \tabref{peaking_bkg_yields}. Since simulated data are used it is necessary to determine
the effective luminosity of that simulated sample and adjust it such that it matches the estimated luminosity of the data. After that, the
number of non-combinatorial background events from the simulated sample is a valid estimate of the one in real
data\footnote{Assuming same total efficiency between the simulation and real data sample.}.

The second and last step of the non-combinatorial background removal is to apply an angular correction factor to account for the fact that
simulated events are distributed uniformly in phase space and hence do not represent a proper decay amplitude structure.
This can cause the non-combinatorial background yield estimations to be wrong because the simulated events are distributed in the $(\Omega, m_{K\pi})$ space
in a different way than the actual non-combinatorial background in the data. The amplitude analysis of \BdJpsipipi, \BsJpsipipi, \BsJpsiKK and \LbJpsipK
which has been performed in\cite{SheldonBdpipi},\cite{SheldonBspipi},\cite{SheldonKK} and\cite{Gao:1701984} respectively provides
the necessary knowledge to compute the correction factors

\begin{equation}
\centering
w_{\rm MC}^i = \frac{P_{\rm DATA}^i (\Omega, m_{hh}  | A_k^i)} {P_{\rm MC}^i(\Omega, m_{hh})},
\end{equation}

\noindent where $P_{\rm DATA}$ and $P_{\rm MC}$ are normalized \pdfs\footnote{\pdf is an acronym for Probability Density Function}.
The index $i$ labels the specie of the particular non-combinatorial background, meaning \BdJpsipipi, \BsJpsipipi, \BsJpsiKK, \LbJpsipK.
Whereas the polarization states, $0$, $\parallel$, $\perp$, of each of the above mentioned species is represented by the $k$ index.
The estimated yields of non-combinatorial background after the above mentioned steps is shown in \tabref{peaking_bkg_yields}

\begin{table}[t]
   \centering
        \begin{tabular}{c c c}
          \hline
          \multicolumn{2}{c}{Sample} & $\pm70\mevcc$ window \\
          \hline
          \multirow{ 2}{*}{\BdJpsipipi} & 2011 & $51 \pm 10$ \\
                                        & 2012 & $115\pm 23$ \\
          \hline
          \multirow{ 2}{*}{\BsJpsipipi} & 2011 & $9.3\pm 2.1$ \\
                                        & 2012 & $25.0\pm 5.4$\\
          \hline
          \multirow{ 2}{*}{\BsJpsiKK}   & 2011 & $10.1 \pm 2.3$ \\
                                        & 2012 & $19.2 \pm 4.0$ \\
          \hline
          \multirow{ 2}{*}{\LbJpsipK}   & 2011 & $36 \pm 17$ \\
                                        & 2012 & $90 \pm 43$ \\
          \hline
          \multirow{ 2}{*}{\LbJpsippi}  & 2011 & $13.8 \pm 5.3$ \\
                                        & 2012 & $27.3 \pm 9.0$ \\
        \hline
        \end{tabular}
        \caption{Estimated non-combinatorial yields in $\pm 70\mevcc$ $K\pi$ mass window of each background
        after re-weighting the angular distributions. The \LbJpsippi events are not re-weighted as explained
        at the end of \secref{peaking_backgrounds}.}
        \label{peaking_bkg_yields}
\end{table}

As it was previously mentioned, the \LbJpsippi background is treated specially. Instead of following the above
procedure, the \LbJpsippi invariant mass line shape is modeled and statistically subtracted in the same way as the
combinatorial background in the next section. The reasons for this different treatment with respect to other
non-combinatorial backgrounds are twofold. First, the full amplitude structure of \LbJpsippi decays was not
sufficiently known\cite{Aaij:2014zoa} when the analysis was concluded and thus the weighting of the simulated samples is not possible.
Second, the peak of the misidentified \LbJpsippi decays in the \jpsi$K\pi$ mass spectrum is broader than those
of the other non-combinatorial backgrounds, see \figref{mass_plot}, as a result the \sPlot technique is still
effective.


% ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
\subsection{sWeighting and Invariant Mass Distribution}
\label{sWeighting_and_mass}

After successfully injecting simulated events with negative weights to cancel non-combinatorial background contributions,
the data sample is effectively composed of, \BdJpsiKpi, \BsJpsiKpi, \LbJpsippi, and combinatorial background not removed by the
multivariate based selection. These four modes are statistically disentangled using the \sPlot technique\cite{splot}.
The technique requires as an input a \pdf that estimates the yield of each
of the above species. The current analysis uses the \pdf of the $\jpsi K\pi$ invariant mass, hereafter just mass, where the yield of each specie
is estimated with an extended maximum likelihood fit to the mass distribution of real data, hereafter mass fit.

\subsubsection{The mass \pdf}
Before discussing the mass fit description it is useful to briefly describe the \pdfs used for each of the following individual species:

\begin{itemize}
\item Combinatorial background: Exponential distribution.
\item \LbJpsippi decays: Amoroso distribution\cite{Amoroso}
\item \Bd and \Bs signals: Hypatia distribution\cite{Santos:2013gra}.
\end{itemize}

\noindent An exponential distribution, $e^{-km}$, is found to be an appropriate description of the combinatorial background. It  accounts
for events where random combinations of final state particles are combined to a signal candidate. Such candidates are not expected to
exhibit any structure in the mass distribution, they simply follow a exponential distribution with a negative slope in the mass spectrum.
The slope is negative because with increasing mass the frequency of those random combinations drops: Since high momentum particles are more
likely to originate from a signal than low momentum ones. This is because the signal particles like the \Bs meson is quite heavy which is
depicted in high momentum of the final state particles that it decays into.

As for the \LbJpsippi background, the Amoroso distribution was found to provide good description of the data.
The parameters of the distribution are obtained from simulated data and then fixed in the mass fit to the data. The first two parameters
are the mean and width of the distribution, whereas the other two are related to the shape. Actually the Amoroso distribution describes
an entire family of distributions. Depending on the values of the shape parameters the resulting distribution can vary from an exponential
to Gaussian distribution. This is a powerful feature that is exploited in order to describe the \LbJpsippi non-combinatorial background shape.

The Hypatia distribution is chosen mainly because it describes the tails of the \Bs and \Bd invariant mass peaks which is
of crucial importance. These two peaks are sufficiently close,
see \figref{mass_plot}, that, in case of bad modeling of the tails, event leakage between the \Bs and \Bd peaks
takes place. Several effects contribute to the structure of the tails apart from mass resolution of the detector. These effects
can be radiative tails (a charged final state particle radiates a photon), interplay of radiative tail with \jpsi mass constraint
or badly reconstructed events caused by decays of the final state hadrons, see\cite{Santos:2013gra}. The tail parameters ($\alpha$, $n$)
are four in total (two for each tail) and are taken from a fit to MC simulated events with a known resolution. The last makes sure that
the tail parameters do not rely on detector simulation imperfections.
For the core of invariant mass distribution, Hypatia uses five shape parameters, namely $\zeta$, $\beta$, $\lambda$, $\sigma$, $\mu$.
The first two are set to zero\footnote{$\zeta$ is empirically found to be very small whereas
$\beta = 0$ implies that the core is symmetric left and right with respect to the mean.}, the third one is taken from the previous
MC simulated sample along with the tail parameters \footnote{In the limit of $\zeta = 0$, $\lambda$ does not depend on detector
effects but only on particle kinematics, the same way as the tail parameters do.} whereas the last two which are the width and
the mean of the core are deterimined by the mass fit.

\subsubsection{The mass fit}
From MC studies, some of the \Bs and \Bd Hypatia parameters appear to be significantly correlated with the \mkpi invariant mass.
Since these parameters need to be fixed in the mass fit, the latter is performed in intervals of the
\mkpi invariant mass. In addition, due to correlations between the mass and one of the variables, $\cos\thetamu$, used in the
subsequent angular fit, the requirements of the \sPlot technique are not satisfied and thus it cannot be applied directly.
Therefore, each \mkpi invariant mass sub-sample is divided further in intervals of $\cos\thetamu$ where the \sPlot technique
can be applied.  The final fit to the data is shown in \figref{mass_plot}, whereas the overall \Bs and \Bd yields are:

\begin{align}
  \centering
  N_{\Bd} &= 208656  \pm  462 ^{+ 78	}_{- 76}, \\
  N_{\Bs} &= \phantom{00}1808  \pm   51 ^{+ 38	}_{- 33} \, ,
  \label{signal_yields}
\end{align}

\noindent where the first uncertainties are statistical and obtained from the quadratic sum of the ones in each fitting category,
and the second correspond to systematical uncertainties. The correlations between the \Bd and \Bs yields in each fitting category
are found to be less than $4\%$. Note that the mass fitting model has been validated with a procedure commonly called {\it toy-study}
similar to the one described in \secref{Toy_Experiments_Study}.

\subsubsection{sWeighting}
Having performed the mass fit all the necessary ingredients to remove the remaining background are in place.
The fitted mass \pdf is now given to the \sPlot algorithm. The algorithm assigns a weight, the so called sWeight, to each candidate
based on the likelihood function built from the input \pdf. The main idea of \sPlot is that for each of the species of the total
\pdf a set of weights can be computed such that they project out only that particular species while effectively subtracting the others.
In the context of the current analysis events that are likely to be background such as combinatorial and \LbJpsipK are
subtracted while \BJpsiKst events are projected with the above mentioned technique, also referred to as sWeighting.

\begin{figure}[t]
  \centering
  \tikzsetnextfilename{mass_plot}
  \scalebox{0.5}{\input{Figures/Chapter4/mass_plot_simul_log}}
  \caption{Invariant mass fit to the data, shown with black points. The green and red curves correspond to the \BdJpsiKst and
           \BsJpsiKst mass \pdf components. The combinatorial and \LbJpsipK background components are shown with balck and cyan
           colors respectively. The total mass \pdf, blue curve, is overlayied to the data. }
  \label{mass_plot}
\end{figure}

The advantage of the \sWeights approach becomes apparent when performing the angular analysis, described in the subsequent section.
Specifically, the resulting weighted candidates can be described by the signal \pdf only, meaning there is no need to explicitly model
the background angular distributions. As a result the fit itself is faster and simpler to implement. On the other hand there are a few
disadvantages such as the evaluation of systematic uncertainties on the estimated angular parameters due to the mass \pdf modeling:
Any variation of these models implies re-computing the \sWeights. Also care needs to be taken such that the uncertainties
of the fitted angular parameters are properly estimated \cite{splot}. The uncertainties tend to be underestimated in a fit with weighted
events. This is because the per event weights in a weighted likelihood fit change the shape of the likelihood function.
The last is where the statistical uncertainty estimation comes from. Thus to correct for this distortion of the likelihood
function shape the scale factor,

\begin{equation}
  \centering
  \alpha = \left(\frac{\sum_{i} w_i}{\sum_{i} w_i^2}\right)^{1/2},
  \label{sWeights_scale_factor}
\end{equation}

\noindent is applied to each weight before performing the angular fit. The scale factor essentially ensures that the sum of weights
equals the number of candidates.
