
There are effectively four stages of offline event selection taking place after the data has been written out. 
First a set of rough pre-selection criteria is applied (hereafter {\it Stripping}). Those cuts are quantities like final state track and vertex quality,
track impact parameter, mass ranges of mother and daughter particles, particle identification and transverse momentum. A detailed 
table of the Stripping cuts is given in \tabref{Bs2JpsiKstSelection}. The purpose of stripping is to perform a first rough rejection of the combinatorial
background and prepare the data for the next stage of selection \footnote{Stripping is also a way to save disk space for offline event storage, since it is impossible
to save anything that the detector writes out}. The remaining three selection steps after Stripping are described in each of the subsequent subsections.

\subsection{Multivariate Based Selection}
\label{Multivariate_Based_Selection}

The \BsJpsiKst signal yield out of the full 3 \invfb data is expected to be low, see \secref{TheBsJpsiKstDecay}. Thus, one would like to reject as much background as possible while keeping all the signal. 
One way to do that would be a cut-based analysis, in which case certain cuts are applied to a set of variables like the \Bs 
mass range for example (essentially resulting in a tighter Stripping). Alternatively a multivariate approach (hear after {\it MVA}) is adopted. 
In that case a set of variables are combined by the MVA algorithm to produce a single variable, the {\it classifying variable}. 
The MVA approach exploits the correlations between input variables get the maximum discriminating power out of their combination.
The calssifing variable ranges from -1 to 1 and classifies each event to be more signal-like (closer to 1) or background-like (closer to -1). 

\begin{figure}[h]
\begin{center}
  \tikzsetnextfilename{bdtg}
  \scalebox{1}{\input{Figures/Chapter4/bdtg}}
  \caption{BDTG distribution on real data. $y$-axis in logarithmic scale}
  \label{BTDG_performance}
\end{center}
\end{figure}

For the current analysis the TMVA toolkit~\cite{TMVA} is used for the MVA selection. In order to construct the classifying variable certain 
input data sets are needed, particularly two sets of a signal-like and a background-like data sets. The first pair is fed to the MVA algorithm 
to construct the classifying variable. In that step the MVA algorithm is "trained" to distinguish between the signal and background input data sets,
this step is called {\it training}. The second pair is used to asses how successful the training step was and it is called {\it testing}. 
For the signal-like samples, \BsJpsiKst Monte-Carlo simulated data (hear after MC)
are used. The \Bs mass window for that sample is $\pm 25 \MeVcc$ around the \Bs peak. As for the background-like sample, candidates from the high mass side-band
with invariant masses between $5401.3\mevcc$ and $5700\mevcc$ are used. Note that the simulated samples are treated exactly the same way as the
real data sample when it comes to any selection cuts applied. A boosted decision tree with gradient boosting (BDTG) is used as the classifying variable. 
The following kinematic variables are provided as input variables for the multivariate procedure (\Bs meson variables are 
named here as \texttt{B0}):

\begin{itemize}
\item{} \texttt{max\_DOCA}: maximum of all distances between pairs of tracks from daughter particles.
\item{} \texttt{B0\_LOKI\_DTF\_CTAU}: time of flight $ct$ of the \Bs meson candidate, where
$t$ is the decay time of the \Bs meson candidate measured in its proper reference frame.
\item{} \texttt{lessIPS}: minimum of all significances on the impact 
                          parameter\footnote{Impact parameter is the vertical distance of track line to a vertex}
                          of a daughter particle (kaons, muons and pions) with respect to the \Bs meson candidate.
\item{} \texttt{B0\_PT}: transverse momentum of the \Bs meson candidate.
\item{} \texttt{B0\_IP\_OWNPV}: impact parameter of the \Bs meson candidate with respect to its best own parent vertex.
\item{} \texttt{B0\_ENDVERTEX\_CHI2}: reconstruction significance of a reconstructed decay vertex of the \Bs meson candidate.
\end{itemize}

The BDTG shows a good discrimination power over signal and background distributions \figref{BTDG_performance}.
Once the training and testing steps are complete a cut on the BDTG is applied so that it maximizes the following figure of merit
(hear after {\it FoM})~\cite{Yuehong_fom}:

\begin{equation}
\label{eqn:fom}
F(w_i) = \frac{\left(\sum{w_{i}}\right)^2}{\sum{w_{i}^2}},
\end{equation}

\noindent where $w_i$ are weights associated to each event (hear after \sWeights), and calculated with the \sPlot technique (see section \secref{sWeighting_and_mass}). 
This FoM can be understood as an effective signal yield, which is inversely proportional to the square root of the total number of events.For a range of BDTG values
a corresponding \sPlot can be performed and a value for the FoM can be obtained. The optimum BDTG value is chosen as the one that maximizes $F(w_i)$.


\begin{figure}[h]
\begin{center}
  \tikzsetnextfilename{mass_before_after_bdtg}
  \scalebox{1}{\input{Figures/Chapter4/mass_before_after_bdtg}}
  \caption{Mass distribution before (black) and after (red) BDTG selection. $y$-axis in logarithmic scale}
  \label{mass_BDTG_selection}
\end{center}
\end{figure}

% ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
\subsection{Peaking Backgrounds}
\label{peaking_backgrounds}

After applying the BTDG cut there is still some combinatorial background remaining which is removed
further using the \sPlot technique described in \secref{sWeighting_and_mass}. However, there is
one more crucial step that is necessary to address prior to the \sPlot technique, namely that of the peaking backgrounds.
Studies of fully simulated samples show contributions from several specific backgrounds, such as \BsJpsiKK, \BsJpsipipi and \BdJpsipipi.
Those backgrounds are the result of assigning the wrong mass hypothesis to a given track during the event reconstruction.  
The invariant mass distribution of misidentified \BdJpsipipi and \BsJpsipipi peaks near the \BsJpsiKpi signal peak
and the misidentified \BsJpsiKK events are located almost under the \BdJpsiKpi signal peak. 
This behavior makes it almost impossible to apply the \sPlot technique. Thus those specific backgrounds need to be removed. 
In order to do that specific simulated events with negative weights are injected to the data such that they cancel out the
contribution of the peaking background events. The current section briefly addresses the treatment of peaking backgrounds 
with negative weights (this is a two step procedure) as well as the special treatment of the \LbJpsippi peaking background.

As a first step towards removing the peaking backgrounds contribution it is necessary to have an estimate on the expected 
the expected peaking backgrounds yields on data. This is done using simulated data and based on the expression:
\begin{equation}
N_{\rm exp} = 2 \times \sigma_{b\bar{b}}\times f_q \times BR \times \varepsilon \times {\mathcal{L}}
\end{equation}
\noindent Where $\sigma_{b\bar{b}}$ is the cross section for the production of a pair of bottom quarks, $f_q$ is the hadronization fraction
(probability that the $b$ quark forms a hadron with another quark type), $\varepsilon$ the total efficiency (reconstruction, selection and trigger)
and ${\mathcal{L}}$ the luminosity of the data. BR stands for the branching fraction of the particular peaking background. Since simulated data are used
it is necessary to estimate the effective luminosity of that simulated sample and scale it to the luminosity of the data. After that the number of
peaking background events from the simulated sample is a valid estimate of the one in real data.

The second and last step of the peaking background removal is to apply an angular correction factor to account for the fact that 
simulated events are generated in phase space\footnote{Phase space generated events use only kinematic variables for the generation
thus there is no angular dependence. On the other hand real data have intrinsic angular dependence} and hence do not contain the proper decay amplitudes.
This can cause the peaking background yield estimations to be inaccurate because the simulated events can be distributed in the $(\Omega, m_{K\pi})$ space
in a different way than the actual peaking background of the data. The amplitude analysis of \BdJpsipipi, \BsJpsipipi, \BsJpsiKK and \LbJpsipK 
which has been performed in~\cite{SheldonBdpipi},~\cite{SheldonBspipi},~\cite{SheldonKK} and~\cite{Gao:1701984} respectively provides enough information
for the amplitude structure of those modes. Therefore the simulated events are weighted with

\begin{equation}
w_{\rm MC} = \frac{P_{\rm DATA} (\Omega, m_{hh}  | {A_i})}{P_{\rm MC}(\Omega, m_{hh})},
\end{equation}

\noindent $P_{\rm DATA}$ and $P_{\rm MC}$ are normalized \pdfs\footnote{Probability Density Function} and $A_i$ stands for
the particular amplitude structure of a certain peaking background mode. The complete description of the above steps can be found in \cite{BsJpsiKst_ANA}.
The final peaking background yields estimation after the above mentioned steps is shown in \tabref{peaking_bkg_yields}

\begin{table}
\begin{center}
%\scriptsize
\begin{tabular}{c|c}%|c|c|c|c}
Sample & $\pm70\mevcc$ window \\
\hline 
\BdJpsipipi 2011 & $51 \pm 10$ \\
\BdJpsipipi 2012 & $115\pm 23$ \\  
\BsJpsipipi 2011 & $9.3\pm 2.1$ \\
\BsJpsipipi 2012 & $25.0\pm 5.4$\\
\BsJpsiKK 2011 & $10.1 \pm 2.3$ \\
\BsJpsiKK 2012 & $19.2 \pm 4.0$ \\ 
\LbJpsipK 2011 & $36 \pm 17$ \\
\LbJpsipK 2012 & $90 \pm 43$ \\ 
%\LbJpsippi 2011 & $13.8 \pm 4.4$ \\
%\LbJpsippi 2012 & $27.3 \pm 6.9$ \\ 
\LbJpsippi 2011 & $13.8 \pm 5.3$ \\
\LbJpsippi 2012 & $27.3 \pm 9.0$ \\
\hline
\end{tabular}
\caption{Approximated expected yields in $\pm 70\mevcc$ $K\pi$ mass window of each background after re-weighting of 
the phase space (the \LbJpsippi decay is not weighted since no amplitude analysis for that decay is published).}
\label{peaking_bkg_yields}
\end{center}
\end{table}


As it was previously mentioned the \LbJpsippi peaking background is treated specially. Instead of following the above
procedure, the \LbJpsippi invariant mass line shape is modeled and statistically removed in the same way as the remaining
combinatorial background in the next section. The reasons for this different treatment with respect to other
peaking backgrounds are two:
\begin{itemize}
\item The full amplitude structure of \LbJpsippi decays is not yet known, and thus the weighting of the simulated samples is not possible. 
\item The peak of the misidentified \LbJpsippi decays in the \Jpsi$K\pi$ mass spectrum is broader than those of the other
      peaking backgrounds see \figref{mass_plot}, making the \sPlot technique to still be effective.  
\end{itemize}

% ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
\subsection{sWeighting and Invariant Mass Distribution}
\label{sWeighting_and_mass}

After successfully injecting simulated events with negative weights in order to remove the peaking background contributions, the data sample is left only with
\BdJpsiKpi, \BsJpsiKpi, \LbJpsippi, and combinatorial background. These four modes (also refereed to as species) are statistically 
disentangled using the \sPlot technique~\cite{splot}. The technique requires as an input a PDF that estimates the yield of each 
of the above specie. The current analysis uses the PDF of the $\Jpsi K\pi$ invariant mass, hereafter just mass, where the yield of each specie
is estimated with an extended maximum likelihood fit to the mass distribution on data, hereafter mass fit. 

\subsubsection{The mass PDF}
Before jumping to the mass fit description it is useful to briefly describe the PDFs of each specie in the next paragraph, which are the following:  

\begin{itemize}
\item Combinatorial background: Exponential decay.
\item \LbJpsippi decays: Amoroso distribution~\cite{Amoroso}
\item \Bd and \Bs signals: Hypatia distribution~\cite{Santos:2013gra}.
\end{itemize}

\noindent An exponential decay distribution, $e^{-km}$, is an appropriate description of the combinatorial background. It correctly accounts for events where
a track from a background processes in that event is associated with the signal mode. This happens due to mistakes in the track reconstruction.
Those kind of events do not exhibit any structure in the mass distribution, they simply follow a exponential distribution with a negative slope in the mass spectrum. 

As for the \LbJpsippi background, the Amoroso distribution, was found to provide good description of the data \figref{LbJpsippi_amoroso}. 
The shape parameters of this distribution are obtained from simulation and then fixed in the mass fit. {\color{red} few more words for those parameters....}

The Hypatia distribution is chosen mainly because it nicely describes the tails of the \Bs and \Bd invariant mass peaks which is
of crucial importance. Those two peaks are close enough,
see picture \figref{mass_plot}, such that in case of bad modeling of the tails, unwanted event leakage between the \Bs and \Bd peaks 
takes place. Several effects contribute to the structure of the tails. Those effects can be radiative tails (a charged final state 
particle radiates a photon), interplay of radiative tail with \Jpsi mass constrain or badly reconstructed events caused by decays 
of the final state hadrons, see~\cite{Santos:2013gra}. The tail parameters are four in total (two for each tail) and are taken
from a fit to MC simulated events with a known resolution. The last makes sure that the tail parameters do not rely on detector 
simulation imperfections. For the core of invariant mass distribution Hypatia uses five shape parameters in total, namely $\zeta$, 
$\beta$, $\lambda$, $\sigma$, $\mu$. The first two are set to zero\footnote{$\zeta$ is empirically found to be very small whereas
$\beta = 0$ implies that the core is symmetric left and right with respect to the mean.}, the third one is taken from the previous
MC simulated sample along with the tail parameters \footnote{In the limit of $\zeta = 0$ $\lambda$ does not depend on detector 
effects but only on particle kinematics, the same way as the tail parameters do.} whereas the last two which are the width and 
the mean of the core are allowed to float in the mass fit.

\begin{table}[!h]
\centering
\begin{tabular}{c c c c}
  \hline
  Bin 0 & Bin 1 & Bin 2 & Bin 3\\
  \hline
  $ [826,861) $ & $ [861,896) $ & $ [896,931) $ & $ [931,966) $ \\
%  $ 826 \leq x \leq 861 $ & $ 861 < x \leq 896 $ & $ 896 < x \leq 931 $ & $ 931 < x \leq 966 $ \\
  \hline
\end{tabular}
\caption{Definitions of \mkpi bins. Units are in \mevcc.}
\label{Kbindef}
\end{table}

\begin{table}[!h]
\centering
\begin{tabular}{c c c c c}
  \hline
  Bin 0 & Bin 1 & Bin 2 & Bin 3 & Bin 4\\	
  \hline
  $ [-1.0,-0.6) $ & $ [-0.6,-0.2) $  & $ [-0.2,0.2) $ & $ [0.2,0.6) $ & $ [0.6,1.0) $ \\
  \hline
 \end{tabular}
\caption{Definitions of $\cos\thetamu$ bins. Units are in rad}
\label{cosThateMubindef} 
\end{table}

\subsubsection{The mass fit}
From MC studies, some of the \Bs and \Bd Hypatia parameters appear to be significantly correlated with the \mkpi invariant mass. 
Since these parameters need to be fixed in the mass fit, the latter is performed in four bins of the
\mkpi invariant mass. In addition, due to correlations between the mass and the cosine of the helicity angle \thetamu\footnote{$\cos\thetamu$ is one of 
the variables used in the subsequent angular fit.},
the requirements of \sPlot technique are not fulfilled and thus cannot be applied. Therefore, each \mkpi invariant mass sub-sample
is divided further in five bins of $\cos\thetamu$, resulting in a total of 20 fitting categories. The \mkpi and $\cos\thetamu$ bins are defined
in \tabref{Kbindef} and \tabref{cosThateMubindef}, respectively. The results of the mass fit is shown in \tabref{massFitData_cosTmuBin0} to
to \tabref{massFitData_cosTmuBin4} and the corresponding plot in \figref{mass_plot}. The overall \Bs and \Bd yields are obtained from the sum 
of yields over the 20 fitting categories, giving

\begin{align}
N_{\Bd} &= 208656  \pm  462 ^{+ 78	}_{- 76}\\
N_{\Bs} &= \phantom{00}1808  \pm   51 ^{+ 38	}_{- 33} \, ,
\label{signal_yields}
\end{align}

\noindent where the first uncertainties are statistical and obtained from the quadratic sum of the ones in each fitting category, 
and the second uncertainties correspond to systematics. The correlations between the \Bd and \Bs yields in each fitting category
are found to be small (smaller than $4\%$).

\begin{figure}[h]
\begin{center}
  \includegraphics[width=0.8\textwidth]{Figures/Chapter4/mass_plot_simul_log.pdf}
  \caption{Invariant mass fit to the data. $y$-axis is in logarithmic scale.}
  \label{mass_plot}
\end{center}
\end{figure}

\subsubsection{sWeighting}
Having performed the mass fit all the necessary ingredients to remove the remaining background are in place. 
The fitted mass PDF is now given to the \sPlot algorithm. The algorithm assigns the so called {\it sWeight}, $w_i$, to each event,
based on the likelihood function built from the input PDF. The main idea of \sPlot is that for each of the species of the total
PDF a set of weights can be computed such that they project only that particular specie on the data while eliminating all the others.  
In the context of the current analysis events that are likely to be background such as combinatorial and \LbJpsipK are
eliminated while \BJpsiKst events are projected with the above mentioned technique (also refered to as sWeighting).

The advantage of the \sWeights approach becomes apparent when performing the angular analysis, described in the subsequent section. Specifically since the resulting 
\BsJpsiKst weighted events are signal only there is no need to model the background shape of the angular distributions. Which
is unknown from first principles. Also the fit itself is faster and simpler to implement. On the other hand there are a few 
disadvantages such as the evaluation of some systematic uncertainties on the fitted angular parameters due to the mass PDF modeling.
Any variation on those models implies re-doing the \sPlot technique. Also special care needs to be taken such that the uncertainties
of the fitted angular parameters are correct \cite{splot}. Particularly The uncertainties tend to be diluted in a fit with weighted
events. This is because the per event weights in a weighted likelihood fit change the shape of the ikelihood function. The last is 
where the statisitcal uncertainty estimation comes from. Thus, a scale factor $\alpha = \left[\sum_{i} w_i /\sum_{i} w_i^2\right]^{1/2}$
has to be applied to each weight to correct for this dilution of the uncertainties before performing the angular fit. The scale factor
essentially forces the sum of weights to be the same as the number of events. For more details see also \secref{Simutaneous_Likelihood_ fit}.

