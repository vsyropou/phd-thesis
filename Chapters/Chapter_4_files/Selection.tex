
With the exception of the trigger, addressed in \secref{det_trigger}, there are effectivelly four stages of offline event
selection which take place after the data has been recorded.
First a set of selection criteria is applied called {\it Stripping}. The purpose of stripping is to roughly
select the triggered data according to their physics content. Furthermore, a loose rejection of the {\it combinatorial}
background is performed such that and the data are prepared for the next selection stages.
Combinatorial background is the type of background typical in particle collision where a large number of
particles is produced. It is caused by random combinations of non-signal particles that build a signal
candidate during the event reconstruction, see \secref{det_tracking}.
Stripping is also a way to save disk space when permanently storing events offline, since it is impossible
to save everything that the detector writes to disk. Each of the stripping selection criteria, also referred to as a {\it cut},
requires an observable to satisfy some condition. For example, the mass of the \Bs candidate must be within
a certain range. Or another type of cut could be the probability that a final state particle, like the pion, is misidentified
as a muon. A detailed table of the Stripping selection is given in \tabref{Bs2JpsiKstSelection}. The remaining three selection
steps after Stripping are described in each of the subsequent subsections.

\subsection{Multivariate Based Selection}
\label{Multivariate_Based_Selection}

The \BsJpsiKst signal yield out of the full 3 \invfb data-set is expected to be small, see \secref{BspsiKst_at_lhcb}. Thus, one would like to
reject as much background as possible while keeping most of the signal. One way to do that would be to tighten the Stripping
selection criteria one by one. Instead, a multivariate approach, hereafter MVA, is adopted.
In that case a set of observables is combined by an MVA algorithm to produce a single variable, the {\it classifier variable}.
The MVA approach can exploit the correlations between input variables to get the maximum discriminating power out of their combination.
The classifying variable is constructed such that it ranges from $-1$ to $+1$. Signal candidates tend to accumulate near $+1$,
whereas background candidates accumulate near $-1$, see \figref{BTDG_performance}.

In the current analysis the MVA lgorithm is a {\it Boosted Decision Tree}, BDT in short, which is a class
of machine learning  algorithm. Further information on the BDT algorithm used can be found in Section 7.2 of \cite{TMVA}.
For the current analysis the TMVA toolkit \cite{TMVA} is used for the MVA selection. In order to construct the classifying variable,
a pair of data sets is needed. One data set must be a representative sample of the signal and one of the background.
This pair of data sets is provided as input to the MVA algorithm to construct the classifying variable.
In that step the MVA algorithm is {\it trained} to distinguish between the signal and background input data sets.
Next another, independent, pair of sets is used to asses how successful the training step was. This step is called {\it testing}.
For the signal representative samples, \BsJpsiKst Monte-Carlo (MC) simulated data are used, hereafter simulated data.
For the background representative sam-

\begin{figure}[!t]
\centering
  \tikzsetnextfilename{bdtg}
  \scalebox{1}{\input{Figures/Chapter4/bdtg}}
  \caption{Data BDT distribution (black points). Signal (blue points) and background (red points) distributions
           tend to accumulate mainly to the right and left respectively.}
  \label{BTDG_performance}
\end{figure}

\noindent -ple real data candidates are used. These candidates are taken from a control
region away from the \BsJpsiKst mass peak, usually referred to as {\it mass sideband}. This way the background representative sample
is not affected by non-perfect simulation and it is also independent of the signal representative sample.
Note that the simulated data are treated by all the processing steps after the \lzero trigger level in the same way as real data.

The two well-separated peaks in \figref{BTDG_performance} suggest that the BDT variable achieves a good
discrimination between signal and background candidates. The BDT was also checked for possible over-training.
The later is a situation where the BDT variable becomes sensitive to statistical fluctuations of the training samples.
This means that statistically compatible distributions, such as the testing and training samples,
appear to have significantly different BDT distributions. This is a not desirable situation which leads to
bad BDT performance. Once the training and testing steps are complete, a cutoff value on the BDT is applied
so that it maximizes the following figure of merit, hereafter FoM:

\begin{equation}
  \centering
  F(w_i) = \frac{\left(\sum{w_{i}}\right)^2}{\sum{w_{i}^2}},
\label{eqn:fom}
\end{equation}

\noindent where $w_i$ are weights associated to each event, hereafter \sWeights, and calculated with the
\sPlot technique. The last  is a statistical tool to unfold data distributions \cite{splot}. More details are provided
in \secref{sWeighting_and_mass}. The above-mentioned FoM can be understood as an effective signal yield.
For a range of BDT values the \sPlot technique provides a different set of \sWeights based on which a different
FoM value can be obtained. The optimum BDT value is chosen such that it maximizes $F(w_i)$.

% ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
\subsection{Reflection Backgrounds}
\label{peaking_backgrounds}

After applying the BTD cut there is still some combinatorial background remaining, see \figref{mass_BDTG_selection},
which is removed further by means of the \sWeights described in \secref{sWeighting_and_mass}.
However, there is one more crucial step that is necessary to address prior to the application of the
\sPlot technique, which is the treatment of {\it reflection} backgrounds. This type of background is due to partcile misidentification.
Specifically a final state particle is asigned the wrong mass hypothesis. As a result the invariant mass of the \Bs candidate,
is shifted with respect to \Bs candidates where all the final state particles were correctly identified.
Another commonly used term for this type of backgound is {\it peaking} background. Studies of simulated samples show contributions
from several specific background sources, such as \BsJpsiKK, \BsJpsipipi and \BdJpsipipi.
These backgrounds are the result of misidentifying a final state particle, for example a pion can
be confused for a kaon. The invariant mass distribution of the above-mentioned misidentified backgrounds
accumulate near the \BJpsiKpi signal peaks. Furthermore given the low number of expected reflection background events,
see \tabref{peaking_bkg_yields}, their distribution is quite non-uniform.
This type of distribution has to be removed before the next selection step can be applied as it is more effective
when the distributions involved are smooth. Therefore, specific simulated decays with negative weights
are injected to the data sample such that they cancel out the contribution of the above-mentioned reflection background
events\footnote{The term "cancel out" obtains proper meaning in the context of the likelihood which is built
in \secref{sWeighting_and_mass} based on the mass probability density function.}.
The current section briefly addresses the treatment of the reflection background with negative weights
as well as the special treatment of the \LbJpsippi reflection background.

\begin{figure}[!t]
  \centering
  \tikzsetnextfilename{mass_before_after_bdtg}
  \scalebox{1}{\input{Figures/Chapter4/mass_before_after_bdtg}}
  \caption{Mass distribution after offline selection (black points), before (blue points) and after (red points) BDT selection.}
  \label{mass_BDTG_selection}
\end{figure}

As a first step towards removing the above-mentioned type of background contribution it is necessary to have an estimate
on the expected reflection background yield on data. This is done using simulated data and based on the expression:
\begin{equation}
N_{\rm exp} = 2 \times \sigma_{b\bar{b}}\times f_q \times \BF \times \varepsilon \times {\mathcal{L}},
\end{equation}
\noindent where $\sigma_{b\bar{b}}$ is the cross section for the production of a pair of bottom quarks, $f_q$ is the hadronization fraction
(probability that the $b$ quark forms a $\Bq$ meson, where $q=\dquark,\squark$), $\varepsilon$ is the total efficiency (including reconstruction,
selection and trigger) and ${\mathcal{L}}$ is the integrated luminosity, hereafter just luminosity, of the data. Finally \BF stands for the
branching fraction of the particular background of \tabref{peaking_bkg_yields}. Since simulated data are used it is necessary to determine
the effective luminosity of that simulated sample and adjust it such that it matches the estimated luminosity of the data.
After that, the number of reflection background events from the simulated sample is a valid estimate of the number of
reflection background events in data\footnote{Assuming same total efficiency between the simulation and real data sample.}.

The second and last step of the reflection background removal is to apply an angular correction factor to account for the fact that
simulated events are distributed uniformly in phase space and hence do not represent a proper decay amplitude structure.
This can cause the reflection background yield estimations to be imperfect because the simulated events are distributed in the $(\Omega, m_{K\pi})$ space
in a different way than the actual reflection background in the data. The amplitude analysis of \BdJpsipipi, \BsJpsipipi, \BsJpsiKK and \LbJpsipK
which has been performed in \cite{SheldonBdpipi},\cite{SheldonBspipi},\cite{SheldonKK} and \cite{Gao:1701984} respectively provides
the necessary knowledge to compute the correction factors

\begin{equation}
\centering
w_{\rm MC}^i = \frac{P_{\rm DATA}^i (\Omega, m_{hh}  | A_k^i)} {P_{\rm MC}^i(\Omega, m_{hh})},
\end{equation}

\noindent where $P_{\rm DATA}$ and $P_{\rm MC}$ are normalized Probability Density Functions, or \pdfs.
The index $i$ labels the species of the particular reflection background, \ie \BdJpsipipi, \BsJpsipipi, \BsJpsiKK, \LbJpsipK.
The polarization states of each of the above species is represented by the $k=\parenthesis{0, \parallel, \perp}$ index.
The estimated yields of reflection background after the above-mentioned steps are shown in \tabref{peaking_bkg_yields}

\begin{table}[t]
   \centering
        \begin{tabular}{c c c}
          \hline
          \multicolumn{2}{c}{Sample} & $\pm70\mevcc$ window \\
          \hline
          \multirow{ 2}{*}{\BdJpsipipi} & 2011 & $51 \pm 10$ \\
                                        & 2012 & $115\pm 23$ \\
          \hline
          \multirow{ 2}{*}{\BsJpsipipi} & 2011 & $9.3\pm 2.1$ \\
                                        & 2012 & $25.0\pm 5.4$\\
          \hline
          \multirow{ 2}{*}{\BsJpsiKK}   & 2011 & $10.1 \pm 2.3$ \\
                                        & 2012 & $19.2 \pm 4.0$ \\
          \hline
          \multirow{ 2}{*}{\LbJpsipK}   & 2011 & $36 \pm 17$ \\
                                        & 2012 & $90 \pm 43$ \\
          \hline
          \multirow{ 2}{*}{\LbJpsippi}  & 2011 & $13.8 \pm 5.3$ \\
                                        & 2012 & $27.3 \pm 9.0$ \\
        \hline
        \end{tabular}
        \caption{Estimated reflection yields in $\pm 70\mevcc$ $K\pi$ mass window of each background
        after re-weighting the angular distributions. The \LbJpsippi events are not re-weighted as explained
        at the end of \secref{peaking_backgrounds}.}
        \label{peaking_bkg_yields}
\end{table}

As it was previously mentioned, the \LbJpsippi background is treated specially. Instead of following the above
procedure, the \LbJpsippi invariant mass line shape is modeled and statistically subtracted in the same way as
the combinatorial background in the next section. The reason for this different treatment with respect to other
reflection backgrounds is due the \LbJpsippi amplitude structure \cite{Aaij:2014zoa}, which was not sufficiently
known when the analysis was concluded. Thus weighting of the simulated samples is not possible.
Now since shape of the misidentified \LbJpsippi decays in the \jpsi$K\pi$ mass spectrum is broad
enough, see \figref{mass_plot}, and the \sPlot technique can be used to remove this specific relfection
background.

% ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
\subsection{sWeighting and Invariant Mass Distribution}
\label{sWeighting_and_mass}

After successfully injecting simulated events with negative weights to cancel reflection background contributions,
the data sample is effectively composed of, \BdJpsiKpi, \BsJpsiKpi, \LbJpsippi, and combinatorial background not removed by the
multivariate based selection. These four modes are statistically disentangled using the \sPlot technique \cite{splot}.
The technique requires as an input a \pdf that estimates the yield of each of the above species. The current analysis uses
a \pdf for the $\jpsi K\pi$ invariant mass, hereafter just mass, where the yield of each species is estimated with an extended
maximum likelihood fit to the mass distribution of data, hereafter mass fit. It is important to point out that the \sPlot
technique assumes no correlation between the mass and other variables-dimension of the data.

\subsubsection{The mass \pdf}
Before discussing the mass fit description it is useful to briefly describe the \pdfs used for each of the following individual species:

\begin{itemize}
\item Combinatorial background: Exponential distribution.
\item \LbJpsippi decays: Amoroso distribution \cite{Amoroso}
\item \Bd and \Bs signals: Hypatia distribution \cite{Santos:2013gra}.
\end{itemize}

\noindent An exponential distribution, $e^{-km}$, is found to be an appropriate description of the combinatorial background. It accounts
for events where random combinations of final state particles are combined to a signal candidate. Such candidates are not expected to
exhibit any structure in the mass distribution, they simply follow an exponential distribution with a negative slope.
The negative slope can be understood when considering two main effects. First, low \Bs invariant mass implies a low momentum of
the particles (\kaon, \pion, \mmu) used to reconstruct the \BsJpsiKst candidate. Second, there are more low momentum partciles produced in
a $\proton-\proton$ colision comapred to high momentum ones. This is because elastic-soft strong interactions between quarks are more
likely to happen compared to "head-on" $\quark-\quark$ ones.

As for the \LbJpsippi background, the Amoroso distribution was found to provide  good description of the data.
The parameters of the distribution are obtained from simulated data and then fixed in the mass fit to the data. The first two parameters
are the mean and width of the distribution, whereas the other two are related to the shape. The Amoroso distribution describes
an entire family of distributions: Depending on the values of the shape parameters the resulting distribution can vary from an exponential
to Gaussian distribution. This is a powerful feature that is exploited in order to describe the \LbJpsippi reflection background shape.

The Hypatia distribution has two main features which makes it a suitable discribion of \Bs and \Bd signal invariant mass distributions.
First, a hyperbolic shape core aims at desctibing the peak of the previous distributions. Second, invariant mass resolution effects are
taken into account by mariginalizing over the \Bq invariant mass uncertainty. Overall the Hypatia function adequatelly
models the tails of the \Bs and \Bd invariant mass distribution which is of crucial importance. This is beacause the previous invariant
mass peaks are sufficiently close, see \figref{mass_plot}, such that bad modeling of the tails implies that a certain number of \Bd decays
are ascociate to \Bs and \viceversa. Several effects contribute to the structure of the tails apart
from mass resolution of the detector. These effects can be radiative tails (a charged final state particle radiates a photon), interplay of
radiative tail with \jpsi mass constraint or badly reconstructed events caused by decays of the final state hadrons, see \cite{Santos:2013gra}.
The tail parameters ($\alpha$, $n$) are four in total (two for each tail) and are taken from a fit to MC simulated events with a known resolution.
The last makes sure that the tail parameters do not rely on detector simulation imperfections.
For the core of invariant mass distribution, Hypatia requires five shape parameters, namely $\zeta$, $\beta$, $\lambda$, $\sigma$, $\mu$.
The first two are set to zero\footnote{$\zeta$ is empirically found to be very small whereas
$\beta = 0$ implies that the core is symmetric left and right with respect to the mean.}, the third one is taken from the previous
MC simulated sample along with the tail parameters \footnote{In the limit of $\zeta = 0$, $\lambda$ does not depend on detector
effects but only on particle kinematics, the same way as the tail parameters do.} whereas the last two which are the width and
the mean of the core are deterimined by the mass fit.

\subsubsection{The mass fit}
From MC studies, some of the \Bs and \Bd Hypatia parameters appear to be significantly correlated with the \mkpi invariant mass.
Since these parameters need to be fixed in the mass fit, the latter is performed in intervals of the
\mkpi invariant mass. In addition, due to correlations between the mass and one of the variables, $\cos\thetamu$, used in
the subsequent angular fit, the requirements of the \sPlot technique are not satisfied\footnote{as explained in the begining
of \secref{sWeighting_and_mass}} and thus it cannot be applied directly.

Therefore, each \mkpi invariant mass sub-sample is divided further in intervals of $\cos\thetamu$ where the \sPlot technique
can be applied. The final fit to the data is shown in \figref{mass_plot}, whereas the overall \Bs and \Bd yields are:

\begin{align}
  \centering
  N_{\Bd} &= 208656  \pm  462 ^{+ 78	}_{- 76}, \\
  N_{\Bs} &= \phantom{00}1808  \pm   51 ^{+ 38	}_{- 33} \, ,
  \label{signal_yields}
\end{align}

\noindent where the first uncertainties are statistical and obtained from the quadratic sum of the ones in each fitting category,
and the second are systematical uncertainties. The correlations between the \Bd and \Bs yields in each fitting category
are found to be less than $4\%$. Note that the mass fitting model has been validated with a procedure commonly called {\it toy-study}.
This procedure checks if the fitting model introduces a bias to the parameters of intereset. Further details can be found in
\secref{Toy_Experiments_Study} where a toy study is perfomred for the angular fitting model of \secref{Total_Decay_Rate}.

\subsubsection{sWeighting}
Having performed the mass fit all the necessary ingredients to remove the remaining background are in place.
The fitted mass \pdf is now given to the \sPlot algorithm. The algorithm assigns a weight, the so called sWeight, to each candidate
based on the likelihood function built from the input \pdf. The main idea of \sPlot is that for each of the species of the total
\pdf a set of weights can be computed such that they project out only that particular species while effectively subtracting the others.
Based on this technique, the \BsJpsiKst events are selected out of the full data sample.

\begin{figure}[t]
  \centering
  \tikzsetnextfilename{mass_plot}
  \scalebox{0.5}{\input{Figures/Chapter4/mass_plot_simul_log}}
  \caption{Invariant mass fit to the data, shown with black points. The green and red curves correspond to the \BdJpsiKst and
           \BsJpsiKst mass \pdf components. The combinatorial and \LbJpsipK background components are shown with balck and cyan
           colors respectively. The total mass \pdf, blue curve, is overlayied to the data. Figure from \cite{bsjpsikst-paper}. }
  \label{mass_plot}
\end{figure}

The advantage of the \sWeights approach becomes apparent when performing the angular analysis, described in the subsequent section.
Specifically, the resulting weighted candidates can be described by the signal \pdf only, meaning there is no need to explicitly model
the background angular distributions. As a result the fit itself is faster and simpler to implement. On the other hand there are a few
disadvantages such as the evaluation of systematic uncertainties on the estimated angular parameters due to the mass \pdf modeling:
Any variation of these models implies re-computing the \sWeights. Also, care must be taken to ensure that the uncertainties
of the fitted angular parameters are properly estimated \cite{splot}. The uncertainties tend to be underestimated in a fit with weighted
events. This is because the per event weights in a weighted likelihood fit change the shape of the likelihood function.
The last is where the statistical uncertainty estimation comes from. Thus to correct for this distortion of the likelihood
function shape the scale factor,

\begin{equation}
  \centering
  \alpha = \left(\frac{\sum_{i} w_i}{\sum_{i} w_i^2}\right)^{1/2},
  \label{sWeights_scale_factor}
\end{equation}

\noindent is applied to each weight before performing the angular fit. The scale factor essentially ensures that the sum of weights
equals the number of candidates.
